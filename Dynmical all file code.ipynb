{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Pandas display options to show all columns and rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read the file without specifying headers\n",
    "file_path = \"/home/choice/Desktop/insurance-fullstack/data/Motor July grid (1).xlsx\"\n",
    "sheet_name = \"ICICI 06_2024- 4W & 2W Grid\"\n",
    "\n",
    "df_initial = pd.read_excel(file_path, sheet_name=sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RTO CATEGORY</th>\n",
       "      <th>RTO Zone</th>\n",
       "      <th>RTO State</th>\n",
       "      <th>RTO Location</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "      <th>Two Wheeler</th>\n",
       "      <th>Unnamed: 13</th>\n",
       "      <th>Unnamed: 14</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "      <th>Unnamed: 17</th>\n",
       "      <th>Unnamed: 18</th>\n",
       "      <th>Unnamed: 19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Private car- New, SAOD,Comp and Used Car      ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Private car AOTP  (Points on Net)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Scooters  (Points on Net)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bikes (Points on net)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EV-TW (Registered &amp; Non Registered)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pvt Car New 1+3</td>\n",
       "      <td>Pvt Car Petrol &amp; CNG- 1+1 (NCB Cases)</td>\n",
       "      <td>Pvt Car Diesel &amp; EV - 1+1 (NCB Cases)</td>\n",
       "      <td>SAOD-NCB</td>\n",
       "      <td>Pvt Car-\\n0 NCB ( NON NCB)</td>\n",
       "      <td>Pvt Car (Used Car**)</td>\n",
       "      <td>Pvt car AOTP- Petrol</td>\n",
       "      <td>Pvt car AOTP- Diesel</td>\n",
       "      <td>TW Scooters- Comp\\n (1+1) (2+2) (3+3)</td>\n",
       "      <td>TW Scooters - (TP only)\\n(0+1) (0+2) (0+3)</td>\n",
       "      <td>TW Scooters- SAOD</td>\n",
       "      <td>TW Bikes Comp (1+1) (2+2) (3+3) (Excluding Baj...</td>\n",
       "      <td>TW Bikes- AOTP \\n(All Manufacturers) \\n(0+1, 0...</td>\n",
       "      <td>TW Bikes -SAOD \\n(All Manufacturers)</td>\n",
       "      <td>Royal Enfield- (SAOD/Comp/ AOTP)</td>\n",
       "      <td>TW - EV (SAOD/Comp/AOTP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Geo</td>\n",
       "      <td>South</td>\n",
       "      <td>ANDHRA PRADESH</td>\n",
       "      <td>Rest of Andhra Pradesh</td>\n",
       "      <td>0.25</td>\n",
       "      <td>25% Maruti and Hyundai, Rest 15%</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.35</td>\n",
       "      <td>20% HMC</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  RTO CATEGORY RTO Zone       RTO State            RTO Location  \\\n",
       "0          NaN      NaN             NaN                     NaN   \n",
       "1          NaN      NaN             NaN                     NaN   \n",
       "2          Geo    South  ANDHRA PRADESH  Rest of Andhra Pradesh   \n",
       "\n",
       "                                          Unnamed: 4  \\\n",
       "0  Private car- New, SAOD,Comp and Used Car      ...   \n",
       "1                                   Pvt Car New 1+3    \n",
       "2                                               0.25   \n",
       "\n",
       "                              Unnamed: 5  \\\n",
       "0                                    NaN   \n",
       "1  Pvt Car Petrol & CNG- 1+1 (NCB Cases)   \n",
       "2       25% Maruti and Hyundai, Rest 15%   \n",
       "\n",
       "                              Unnamed: 6 Unnamed: 7  \\\n",
       "0                                    NaN        NaN   \n",
       "1  Pvt Car Diesel & EV - 1+1 (NCB Cases)   SAOD-NCB   \n",
       "2                                   0.15      0.225   \n",
       "\n",
       "                   Unnamed: 8            Unnamed: 9  \\\n",
       "0                         NaN                   NaN   \n",
       "1  Pvt Car-\\n0 NCB ( NON NCB)  Pvt Car (Used Car**)   \n",
       "2                        0.15                  0.15   \n",
       "\n",
       "                         Unnamed: 10           Unnamed: 11  \\\n",
       "0  Private car AOTP  (Points on Net)                   NaN   \n",
       "1               Pvt car AOTP- Petrol  Pvt car AOTP- Diesel   \n",
       "2                               0.25                  0.05   \n",
       "\n",
       "                             Two Wheeler  \\\n",
       "0              Scooters  (Points on Net)   \n",
       "1  TW Scooters- Comp\\n (1+1) (2+2) (3+3)   \n",
       "2                                    0.4   \n",
       "\n",
       "                                  Unnamed: 13        Unnamed: 14  \\\n",
       "0                                         NaN                NaN   \n",
       "1  TW Scooters - (TP only)\\n(0+1) (0+2) (0+3)  TW Scooters- SAOD   \n",
       "2                                         0.1               0.35   \n",
       "\n",
       "                                         Unnamed: 15  \\\n",
       "0                              Bikes (Points on net)   \n",
       "1  TW Bikes Comp (1+1) (2+2) (3+3) (Excluding Baj...   \n",
       "2                                            20% HMC   \n",
       "\n",
       "                                         Unnamed: 16  \\\n",
       "0                                                NaN   \n",
       "1  TW Bikes- AOTP \\n(All Manufacturers) \\n(0+1, 0...   \n",
       "2                                               0.15   \n",
       "\n",
       "                            Unnamed: 17                        Unnamed: 18  \\\n",
       "0                                   NaN                                NaN   \n",
       "1  TW Bikes -SAOD \\n(All Manufacturers)  Royal Enfield- (SAOD/Comp/ AOTP)    \n",
       "2                                   0.3                                  0   \n",
       "\n",
       "                           Unnamed: 19  \n",
       "0  EV-TW (Registered & Non Registered)  \n",
       "1             TW - EV (SAOD/Comp/AOTP)  \n",
       "2                                  0.3  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_initial.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the first row that appears to be part of the table structure\n",
    "def find_table_start(df):\n",
    "    for idx, row in df.iterrows():\n",
    "        if row.notnull().sum() >= 2 and not all(row.astype(str).str.contains('^Unnamed', na=False)):  # At least 2 non-null values and not all 'Unnamed'\n",
    "            return idx\n",
    "    return None\n",
    "\n",
    "## to find the first numerical row \n",
    "def find_first_numerical_row(df):\n",
    "    for idx, row in df.iterrows():\n",
    "        # Check if any value in the row is numerical (int, float) or a digit-like string\n",
    "        if row.apply(lambda x: not pd.isna(x) and (isinstance(x, (int, float)) or (isinstance(x, str) and (x.isdigit() or (x.endswith('%')))))).any():\n",
    "            return idx\n",
    "    return None\n",
    "\n",
    "# to find the name of the column from which the numerical value start\n",
    "# def find_row_with_most_numerical_values(df):\n",
    "#     max_count = 0\n",
    "#     row_with_most_numericals = None\n",
    "#     first_numerical_column_name = None\n",
    "    \n",
    "#     for idx, row in df.iterrows():\n",
    "#         # Check if any value in the row is numerical (int, float) or a digit-like string\n",
    "#         is_numerical = row.apply(lambda x: not pd.isna(x) and (isinstance(x, (int, float)) or (isinstance(x, str) and x.isdigit())))\n",
    "        \n",
    "#         # Count the number of numerical values in the row\n",
    "#         numerical_count = is_numerical.sum()\n",
    "        \n",
    "#         if numerical_count > max_count:\n",
    "#             max_count = numerical_count\n",
    "#             row_with_most_numericals = idx\n",
    "            \n",
    "#             # Find the first column name with a numerical value in this row\n",
    "#             first_numerical_column = row.index[is_numerical]\n",
    "#             if len(first_numerical_column) > 0:\n",
    "#                 first_numerical_column_name = first_numerical_column[0]\n",
    "#             else:\n",
    "#                 first_numerical_column_name = None\n",
    "    \n",
    "#     return row_with_most_numericals, first_numerical_column_name\n",
    "\n",
    "def extract_numerical_value(value):\n",
    "    # Regular expression to match numeric values, optional space after '-', and optional '%' at the end\n",
    "    pattern = re.compile(r'-\\s*\\d+(\\.\\d+)?%?$|^\\d+(\\.\\d+)?%?$')\n",
    "    match = pattern.search(str(value))\n",
    "    return match.group() if match else None\n",
    "\n",
    "def find_row_with_most_numerical_values(df):\n",
    "    max_count = 0\n",
    "    row_with_most_numericals = None\n",
    "    first_numerical_column_name = None\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        # Extract numerical parts from each cell if present\n",
    "        is_numerical = row.apply(lambda x: not pd.isna(x) and (\n",
    "            isinstance(x, (int, float)) or\n",
    "            extract_numerical_value(x) is not None\n",
    "        ))\n",
    "        \n",
    "        # Count the number of numerical values in the row\n",
    "        numerical_count = is_numerical.sum()\n",
    "        \n",
    "        if numerical_count > max_count:\n",
    "            max_count = numerical_count\n",
    "            row_with_most_numericals = idx\n",
    "            \n",
    "            # Find the first column name with a numerical value in this row\n",
    "            first_numerical_column = row.index[is_numerical]\n",
    "            # print(first_numerical_column)\n",
    "            if len(first_numerical_column) > 0:\n",
    "                if first_numerical_column[0] == df.columns[0]:\n",
    "                    first_numerical_column_name = first_numerical_column[1] if len(first_numerical_column) > 1 else None\n",
    "                else:\n",
    "                    first_numerical_column_name = first_numerical_column[0]\n",
    "                # print(first_numerical_column_name)\n",
    "            else:\n",
    "                first_numerical_column_name = None\n",
    "    \n",
    "    return row_with_most_numericals, first_numerical_column_name\n",
    "\n",
    "\n",
    "# def find_all_numerical_columns(df):\n",
    "#     numerical_columns = []\n",
    "\n",
    "#     # Iterate over each column\n",
    "#     for col in df.columns:\n",
    "#         # Identify the values in the column that match the condition\n",
    "#         matching_mask = df[col].apply(\n",
    "#             lambda x: not pd.isna(x) and (\n",
    "#                 isinstance(x, (int, float)) or\n",
    "#                 (isinstance(x, str) and (x.isdigit() or (x.endswith('%'))))\n",
    "#             )\n",
    "#         )\n",
    "#         if matching_mask.any():\n",
    "#             numerical_columns.append(col)\n",
    "    \n",
    "#     # Return the first numerical column if any are found, along with the list of all numerical columns\n",
    "#     if numerical_columns:\n",
    "#         first_numerical_column = numerical_columns[0]\n",
    "#         return first_numerical_column, numerical_columns\n",
    "#     else:\n",
    "#         return None, []\n",
    "\n",
    "def clean_text(x):\n",
    "    if isinstance(x, str):\n",
    "        # Replace newline characters with a space\n",
    "        x = x.replace('\\n', ' ').replace('\\r', ' ')\n",
    "        # Remove extra spaces (multiple spaces replaced by a single space)\n",
    "        x = ' '.join(x.split())\n",
    "    return x\n",
    "\n",
    "def clean_column_headers(df):\n",
    "    new_columns = []\n",
    "    for col in df.columns:\n",
    "        if col.startswith('Unnamed'):\n",
    "            # Strip 'Unnamed' prefix and leading/trailing spaces\n",
    "            new_col = col.split('__', 1)[-1].strip()\n",
    "        else:\n",
    "            new_col = col\n",
    "        new_columns.append(new_col)\n",
    "    df.columns = new_columns\n",
    "    return df\n",
    "    \n",
    "\n",
    "# Function to clean the text inside parentheses\n",
    "def clean_parentheses_content(match):\n",
    "    content = match.group(1)\n",
    "    # cleaned_content = re.sub(r'[^\\w\\s]', '', content)  # Remove unwanted symbols\n",
    "    cleaned_content = re.sub(r'\\*', '', content)\n",
    "    cleaned_content = cleaned_content.strip()  # Remove leading and trailing spaces\n",
    "    return f\"_{cleaned_content}\"\n",
    "\n",
    "# Function to process text\n",
    "def replace_brackets(text):\n",
    "    if isinstance(text, str):\n",
    "        # Replace parentheses and remove unwanted symbols within them\n",
    "        text = re.sub(r'\\((.*?)\\)', clean_parentheses_content, text)\n",
    "        # Remove extra spaces around underscores\n",
    "        text = re.sub(r'\\s*_\\s*', '_', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Function to find the common prefix among a list of strings\n",
    "def common_prefix(strings):\n",
    "    if not strings:\n",
    "        return \"\"\n",
    "    # Start with the first string in the list as the common prefix\n",
    "    prefix = strings[0]\n",
    "    for s in strings[1:]:\n",
    "        # Compare the current prefix with each string\n",
    "        while s[:len(prefix)] != prefix and prefix:\n",
    "            # Reduce the prefix by one character at a time\n",
    "            prefix = prefix[:-1]\n",
    "    return prefix\n",
    "\n",
    "# Function to rename columns with condition\n",
    "def rename_columns(df):\n",
    "    cols = list(df.columns)\n",
    "    # Exclude columns that start with the exclude_prefix from prefix comparison\n",
    "    cols_to_check = [col for col in cols if not col.startswith('variable')]\n",
    "    \n",
    "    common_prefix_str = common_prefix(cols_to_check)\n",
    "    \n",
    "    new_cols = []\n",
    "    for col in cols:\n",
    "        if col.startswith('variable'):\n",
    "            new_cols.append(col)\n",
    "        elif col.startswith(common_prefix_str):\n",
    "            new_cols.append(col[len(common_prefix_str):])\n",
    "        else:\n",
    "            new_cols.append\n",
    "    # print(new_cols)\n",
    "    return new_cols\n",
    "\n",
    "\n",
    "def strip_spaces(text):\n",
    "    if isinstance(text, str):\n",
    "        return text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def find_last_numerical_row(df, exclude_columns=[]):\n",
    "    for idx, row in df.iloc[::-1].iterrows():\n",
    "        # Filter out the excluded columns\n",
    "        filtered_row = row.drop(labels=exclude_columns)\n",
    "        # Check if any value in the filtered row is numerical (int, float) or a digit-like string\n",
    "        if filtered_row.apply(lambda x: not pd.isna(x) and (isinstance(x, (int, float)) or (isinstance(x, str) and (x.isdigit() or x.endswith('%'))))).any():\n",
    "            return idx\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "# def find_all_prefixes(columns):\n",
    "#     exclude_prefixes = 'variable'\n",
    "#     prefixes = set()\n",
    "#     for col in columns:\n",
    "#         if any(col.startswith(exclude) for exclude in exclude_prefixes):\n",
    "#             continue\n",
    "#         parts = col.split('_')\n",
    "#         for i in range(1, len(parts)):\n",
    "#             prefix = '_'.join(parts[:i]) + '_'\n",
    "#             prefixes.add(prefix)\n",
    "#     return prefixes\n",
    "\n",
    "# def remove_prefixes(df, prefixes):\n",
    "#     # exclude_prefixes = 'variable'\n",
    "#     new_columns = []\n",
    "#     for col in df.columns:\n",
    "#         new_col = col\n",
    "#         for prefix in prefixes:\n",
    "#             new_col = new_col.replace(prefix, '')\n",
    "#         if new_col.startswith('_'):\n",
    "#             new_col = new_col.lstrip('_')\n",
    "#         new_columns.append(new_col)\n",
    "#     df.columns = new_columns\n",
    "#     # print(new_columns)\n",
    "#     return df\n",
    "\n",
    "\n",
    "def find_all_prefixes(columns, exclude_prefixes=['variable']):\n",
    "    prefixes = set()\n",
    "    for col in columns:\n",
    "        if any(col.startswith(exclude) for exclude in exclude_prefixes):\n",
    "            continue\n",
    "        parts = col.split('__')\n",
    "        for i in range(1, len(parts)):\n",
    "            prefix = '__'.join(parts[:i]) + '__'\n",
    "            prefixes.add(prefix)\n",
    "    return prefixes\n",
    "\n",
    "def remove_prefixes(df, prefixes):\n",
    "    new_columns = []\n",
    "    for col in df.columns:\n",
    "        new_col = col\n",
    "        for prefix in sorted(prefixes, key=len, reverse=True):  # Sort by length to avoid partial replacements\n",
    "            if new_col.startswith(prefix):\n",
    "                new_col = new_col[len(prefix):]\n",
    "                break  # Only remove the longest matching prefix\n",
    "        new_columns.append(new_col)\n",
    "    df.columns = new_columns\n",
    "    return df\n",
    "\n",
    "\n",
    "def find_last_numerical_row_fordrop(df):\n",
    "    for idx, row in df.iloc[::-1].iterrows():\n",
    "        # Check if any value in the row is numerical (int, float) or a digit-like string\n",
    "        if row.apply(lambda x: not pd.isna(x) and (isinstance(x, (int, float)) or (isinstance(x, str) and (x.isdigit() or x.endswith('%'))))).any():\n",
    "            return idx\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10192/4256712525.py:379: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  above_rows_filled = above_rows.fillna(method='ffill', axis=1)\n",
      "/tmp/ipykernel_10192/4256712525.py:379: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  above_rows_filled = above_rows.fillna(method='ffill', axis=1)\n",
      "/tmp/ipykernel_10192/4256712525.py:401: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  new_df = new_df.applymap(clean_text) ## for multiline text or wrap text in row or cell\n",
      "/tmp/ipykernel_10192/4256712525.py:415: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  new_df[columns_before] = new_df[columns_before].fillna(method='ffill')\n",
      "/tmp/ipykernel_10192/4256712525.py:466: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  melted_df = melted_df.applymap(replace_brackets)\n",
      "/tmp/ipykernel_10192/4256712525.py:469: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  melted_df = melted_df.applymap(strip_spaces)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Check the first numerical row index of df_initial before converting and preprocess the data\n",
    "first_num_row_condition = find_first_numerical_row(df_initial) \n",
    "\n",
    "if first_num_row_condition > 0:\n",
    "    # Step 2: Check if all initial headers are 'Unnamed'\n",
    "    if all(df_initial.columns.astype(str).str.contains('^Unnamed', na=False)):\n",
    "        # All initial headers are 'Unnamed', find the table start row using df_initial\n",
    "        table_start_idx = find_table_start(df_initial)\n",
    "        \n",
    "        # If table_start_idx is None, there is no valid table structure in the file\n",
    "        if table_start_idx is None:\n",
    "            raise ValueError(\"No valid table structure found in the file.\")\n",
    "        \n",
    "        # Adjust df_initial to start from the detected table start row\n",
    "        df = df_initial.iloc[table_start_idx + 1:].reset_index(drop=True)\n",
    "\n",
    "        # print(df)\n",
    "        df.columns = df_initial.iloc[table_start_idx]\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        # # Get the current column names\n",
    "        columns = df.columns\n",
    "\n",
    "        # # Create a list to store the new column names with forward fill\n",
    "        new_columns = [columns[0]]  # Start with the first column name\n",
    "\n",
    "        # Iterate through the columns and forward fill wherer the column name is null or nan for this case only\n",
    "        for col in columns[1:]:\n",
    "            if pd.isna(col):\n",
    "                new_columns.append(new_columns[-1])  # Use the last valid column name\n",
    "            else:\n",
    "                new_columns.append(col)\n",
    "\n",
    "        # # Assign the new column names to the DataFrame\n",
    "        df.columns = new_columns       \n",
    "\n",
    "        #         # Step 2: Initialize a list to store notes\n",
    "        # notes = []\n",
    "        # #print(notes)\n",
    "\n",
    "        # # Step 3: Iterate through each row in the DataFrame to separate notes and remove them from df\n",
    "        # index_to_drop = []  # To store indices of rows to drop from df\n",
    "        # # for index, row in df.iterrows():\n",
    "        # #     if pd.notna(row[0]) and not any(row.dropna().index[1:]):  # Check if it's a note row\n",
    "        # #         notes.append(row[0])  # Append note to notes list\n",
    "        # #         index_to_drop.append(index)  # Mark row index to drop\n",
    "\n",
    "        # for index, row in df.iterrows():\n",
    "        #     if (pd.notna(row[0]) or pd.notna(row[1])) and not any(row.dropna().index[2:]):  # Check if it's a note row\n",
    "        #         notes.append(row[0] if pd.notna(row[0]) else row[1])  # Append note to notes list\n",
    "        #         index_to_drop.append(index) \n",
    "\n",
    "        # # Step 4: Keep rows up to and including the smallest index in index_to_drop\n",
    "        # if index_to_drop:\n",
    "        #     min_index = min(index_to_drop)  # Find the smallest index to keep\n",
    "        #     df = df.iloc[:min_index].reset_index(drop=True)  # Keep rows from the beginning up to but not including this index\n",
    "\n",
    "        # # Step 2: Drop rows with all NaN values\n",
    "        df = df.dropna(how='all').reset_index(drop=True)\n",
    "\n",
    "        # # Drop columns where all values are NaN\n",
    "        df = df.dropna(axis=1, how='all')\n",
    "\n",
    "        # Step 3: Find the first numerical row\n",
    "        first_num_row_idx = find_first_numerical_row(df)\n",
    "\n",
    "        last_numerical_idx = find_last_numerical_row(df)\n",
    "\n",
    "\n",
    "\n",
    "        if last_numerical_idx is not None:\n",
    "                df = df.iloc[:last_numerical_idx + 1]\n",
    "\n",
    "        #         # # Step 2: Drop rows with all NaN values\n",
    "        # df = df.dropna(how='all').reset_index(drop=True)\n",
    "\n",
    "        # # # Drop columns where all values are NaN\n",
    "        # df = df.dropna(axis=1, how='all')\n",
    "\n",
    "            # Process only if the first numerical row is not the first row of the DataFrame\n",
    "        if first_num_row_idx > 0:\n",
    "            # Step 4: Extract original headers and rows above the first numerical row\n",
    "            original_headers = df.columns.tolist()\n",
    "            above_rows = df.iloc[:first_num_row_idx]\n",
    "\n",
    "            ## for the Filling null value in row by last valide value to get proper header \n",
    "            above_rows_filled = above_rows.fillna(method='ffill', axis=1)\n",
    "\n",
    "            ## Assign the column header propely to use for the further work\n",
    "            df.iloc[:first_num_row_idx] = above_rows_filled\n",
    "\n",
    "            # Collect all values from above rows into a single list\n",
    "            above_rows_values = above_rows.values.flatten().tolist()\n",
    "\n",
    "            # Create combined headers by appending values above to the original headers\n",
    "            # combined_headers = [f\"{str(header)}_{str(value)}\" if not pd.isna(value) else header for header, value in zip(original_headers, above_rows_values)]\n",
    "            \n",
    "            ## below code to  handle the  if single or multiple row under the above rows to create proper Header of columns\n",
    "            combined_headers = original_headers[:]  # Start with original headers\n",
    "            for idx, row in above_rows.iterrows():\n",
    "                combined_headers = [\n",
    "                    f\"{header}__{str(value)}\" if not pd.isna(value) else header\n",
    "                    for header, value in zip(combined_headers, row)\n",
    "                ]\n",
    "\n",
    "            # Create new DataFrame with combined headers\n",
    "            new_df = pd.DataFrame(df.iloc[first_num_row_idx:].values, columns=combined_headers)\n",
    "\n",
    "            new_df = new_df.applymap(clean_text) ## for multiline text or wrap text in row or cell\n",
    "\n",
    "            # Apply the cleaning function to the column names\n",
    "            new_df.columns = [clean_text(col) for col in new_df.columns] ## removing text of multiline and wrap from heading\n",
    "\n",
    "            # Replace NaN column headers with 'type'\n",
    "            new_df.columns = ['Policy_type' if pd.isna(col) else col for col in new_df.columns]\n",
    "\n",
    "            new_df = clean_column_headers(new_df)\n",
    "\n",
    "            # Call the function\n",
    "            first_numerical_row_idx, columns_with_numerical_values = find_row_with_most_numerical_values(new_df)\n",
    "            # columns_with_numerical_values, all_numerical_columns = find_all_numerical_columns(new_df)\n",
    "            \n",
    "            col_idx = new_df.columns.get_loc(columns_with_numerical_values)\n",
    "            columns_before = new_df.columns[:col_idx]\n",
    "\n",
    "            # Step 1: Identify duplicated columns\n",
    "            duplicate_columns = []\n",
    "\n",
    "            for col in columns_before:\n",
    "                if col in new_df.columns:\n",
    "                    for other_col in new_df.columns:\n",
    "                        if col != other_col and new_df[col].equals(new_df[other_col]):\n",
    "                            duplicate_columns.append(other_col)\n",
    "\n",
    "            # Step 2: Drop duplicate columns\n",
    "            new_df.drop(columns=duplicate_columns, inplace=True)\n",
    "\n",
    "            new_df[columns_before] = new_df[columns_before].fillna(method='ffill')\n",
    "\n",
    "            last_numerical_row = find_last_numerical_row(new_df, columns_before)\n",
    "\n",
    "            \n",
    "\n",
    "            for col in new_df.columns[col_idx:]:\n",
    "                new_df[col] = new_df[col].apply(lambda x: f\"{float(x) * 100:.2f}\" if isinstance(x, (int, float)) and str(x).startswith('0.') else x)  #% symbol removing and value at time of read convert to decimal reconvert to same\n",
    "\n",
    "\n",
    "            # new_df = new_df.applymap(clean_text) ## for multiline text or wrap text in row or cell\n",
    "\n",
    "            # # Apply the cleaning function to the column names\n",
    "            # new_df.columns = [clean_text(col) for col in new_df.columns] ## removing text of multiline and wrap from heading\n",
    "\n",
    "                # Loop through the columns and rename if they start with 'All_Broker_'\n",
    "            new_columns = {col: col.split('__')[0] for col in new_df.columns if col.lower().startswith('all broker -')}\n",
    "\n",
    "            # Rename the columns in the DataFrame\n",
    "            new_df.rename(columns=new_columns, inplace=True)\n",
    "\n",
    "            # Melt the DataFrame\n",
    "            value_vars = new_df.columns[col_idx:]\n",
    "\n",
    "            melted_df = pd.melt(new_df, id_vars=list(columns_before), value_vars=value_vars, var_name='variable', value_name='commission')\n",
    "\n",
    "            # Split the 'variable' column based on '/' and '_' and expand into separate columns\n",
    "            # expanded_cols = melted_df['variable'].str.split(r'[_]', expand=True)\n",
    "            expanded_cols = melted_df['variable'].str.split(r'__', expand=True)\n",
    "            \n",
    "            # Rename columns based on the maximum number of splits\n",
    "            expanded_cols.columns = [f'variable_{i+1}' for i in range(expanded_cols.shape[1])]\n",
    "            \n",
    "            # Concatenate the expanded columns to the melted DataFrame\n",
    "            melted_df = pd.concat([melted_df, expanded_cols], axis=1)\n",
    "            \n",
    "            # Rearrange columns: expanded columns after id_vars and before 'commission'\n",
    "            id_vars = list(columns_before)\n",
    "            value_name = 'commission'\n",
    "            \n",
    "            columns_order = id_vars + [col for col in expanded_cols.columns] + [value_name]\n",
    "            melted_df = melted_df[columns_order]\n",
    "\n",
    "\n",
    "                        # Remove rows where the commission value is a substring of any value in the id_vars columns\n",
    "            rows_to_drop = melted_df.apply(\n",
    "                lambda row: any(str(row[value_name]) in str(row[id_var]) for id_var in id_vars), axis=1)\n",
    "\n",
    "            melted_df = melted_df[~rows_to_drop].reset_index(drop=True)\n",
    "\n",
    "            \n",
    "            melted_df.columns = [col.split('__')[-1] if col.lower().startswith('all broker') else col for col in melted_df.columns]\n",
    "            melted_df= melted_df.dropna(subset=['commission']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "            melted_df.columns = rename_columns(melted_df)\n",
    "            # Apply the function to the entire DataFrame\n",
    "            melted_df = melted_df.applymap(replace_brackets)\n",
    "\n",
    "            # # Remove rows where any value in the 'commission' column matches any value in the 'id_vars' columns\n",
    "            # rows_to_drop = melted_df.apply(\n",
    "            #     lambda row: row[id_vars].isin([row[value_name]]).any(), axis=1)\n",
    "\n",
    "            # melted_df = melted_df[~rows_to_drop].reset_index(drop=True)\n",
    "\n",
    "            # Apply the function to the entire DataFrame\n",
    "            melted_df = melted_df.applymap(strip_spaces)\n",
    "\n",
    "\n",
    "            # Find all possible prefixes excluding certain prefixes\n",
    "            all_prefixes = find_all_prefixes(melted_df.columns)\n",
    "            # Remove all identified prefixes\n",
    "            melted_df = remove_prefixes(melted_df, all_prefixes)\n",
    "\n",
    "        \n",
    "        else:\n",
    "            # Step 2: Drop rows with all NaN values\n",
    "            df = df.dropna(how='all').reset_index(drop=True)\n",
    "\n",
    "            # Drop columns where all values are NaN\n",
    "            df = df.dropna(axis=1, how='all')\n",
    "\n",
    "            # # Step 2: Initialize a list to store notes\n",
    "            # notes = []\n",
    "            # #print(notes)\n",
    "\n",
    "            # # Step 3: Iterate through each row in the DataFrame to separate notes and remove them from df\n",
    "            # index_to_drop = []  # To store indices of rows to drop from df\n",
    "            # # for index, row in df.iterrows():\n",
    "            # #     if pd.notna(row[0]) and not any(row.dropna().index[1:]):  # Check if it's a note row\n",
    "            # #         notes.append(row[0])  # Append note to notes list\n",
    "            # #         index_to_drop.append(index)  # Mark row index to drop\n",
    "\n",
    "            # for index, row in df.iterrows():\n",
    "            #     if (pd.notna(row[0]) or pd.notna(row[1])) and not any(row.dropna().index[2:]):  # Check if it's a note row\n",
    "            #         notes.append(row[0] if pd.notna(row[0]) else row[1])  # Append note to notes list\n",
    "            #         index_to_drop.append(index) \n",
    "\n",
    "            # # Step 4: Keep rows up to and including the smallest index in index_to_drop\n",
    "            # if index_to_drop:\n",
    "            #     min_index = min(index_to_drop)  # Find the smallest index to keep\n",
    "            #     df = df.iloc[:min_index].reset_index(drop=True)  # Keep rows from the beginning up to but not including this index\n",
    "\n",
    "\n",
    "            new_df = df.copy()\n",
    "\n",
    "            new_df = new_df.applymap(clean_text) ## for multiline text or wrap text in row or cell\n",
    "\n",
    "            # Apply the cleaning function to the column names\n",
    "            new_df.columns = [clean_text(col) for col in new_df.columns] ## removing text of multiline and wrap from heading\n",
    "\n",
    "\n",
    "            new_df = clean_column_headers(new_df)\n",
    "\n",
    "            # Call the function\n",
    "            first_numerical_row_idx, columns_with_numerical_values = find_row_with_most_numerical_values(new_df)\n",
    "            # columns_with_numerical_values, all_numerical_columns = find_all_numerical_columns(new_df)\n",
    "            \n",
    "            col_idx = new_df.columns.get_loc(columns_with_numerical_values)\n",
    "            columns_before = new_df.columns[:col_idx]\n",
    "\n",
    "            new_df[columns_before] = new_df[columns_before].fillna(method='ffill')\n",
    "\n",
    "            last_numerical_row = find_last_numerical_row(new_df, columns_before)\n",
    "\n",
    "            for col in new_df.columns[col_idx:]:\n",
    "                new_df[col] = new_df[col].apply(lambda x: f\"{float(x) * 100:.2f}\" if isinstance(x, (int, float)) and str(x).startswith('0.') else x)  #% symbol removing and value at time of read convert to decimal reconvert to same\n",
    "\n",
    "\n",
    "            # new_df = new_df.applymap(clean_text) ## for multiline text or wrap text in row or cell\n",
    "\n",
    "            # # Apply the cleaning function to the column names\n",
    "            # new_df.columns = [clean_text(col) for col in new_df.columns] ## removing text of multiline and wrap from heading\n",
    "\n",
    "\n",
    "                # Loop through the columns and rename if they start with 'All_Broker_'\n",
    "            new_columns = {col: col.split('__')[0] for col in df.columns if col.lower().startswith('all broker -')}\n",
    "\n",
    "            # Rename the columns in the DataFrame\n",
    "            new_df.rename(columns=new_columns, inplace=True)\n",
    "\n",
    "            # Melt the DataFrame\n",
    "            value_vars = new_df.columns[col_idx:]\n",
    "            melted_df = pd.melt(new_df, id_vars=list(columns_before), value_vars=value_vars, var_name='variable', value_name='commission')\n",
    "\n",
    "            # Split the 'variable' column based on '/' and '_' and expand into separate columns\n",
    "            # expanded_cols = melted_df['variable'].str.split(r'[_]', expand=True)\n",
    "            expanded_cols = melted_df['variable'].str.split(r'__', expand=True)\n",
    "            \n",
    "            # Rename columns based on the maximum number of splits\n",
    "            expanded_cols.columns = [f'variable_{i+1}' for i in range(expanded_cols.shape[1])]\n",
    "            \n",
    "            # Concatenate the expanded columns to the melted DataFrame\n",
    "            melted_df = pd.concat([melted_df, expanded_cols], axis=1)\n",
    "            \n",
    "            # Rearrange columns: expanded columns after id_vars and before 'commission'\n",
    "            id_vars = list(columns_before)\n",
    "            value_name = 'commission'\n",
    "            \n",
    "            columns_order = id_vars + [col for col in expanded_cols.columns] + [value_name]\n",
    "            melted_df = melted_df[columns_order]\n",
    "\n",
    "\n",
    "                        # Remove rows where the commission value is a substring of any value in the id_vars columns\n",
    "            rows_to_drop = melted_df.apply(\n",
    "                lambda row: any(str(row[value_name]) in str(row[id_var]) for id_var in id_vars), axis=1)\n",
    "\n",
    "            melted_df = melted_df[~rows_to_drop].reset_index(drop=True)\n",
    "\n",
    "            melted_df.columns = [col.split('__')[-1] if col.lower().startswith('all broker') else col for col in melted_df.columns]\n",
    "            melted_df= melted_df.dropna(subset=['commission']).reset_index(drop=True) \n",
    "\n",
    "            melted_df.columns = rename_columns(melted_df) \n",
    "            # Apply the function to the entire DataFrame\n",
    "            melted_df = melted_df.applymap(replace_brackets)\n",
    "\n",
    "            # Apply the function to the entire DataFrame\n",
    "            melted_df = melted_df.applymap(strip_spaces)\n",
    "\n",
    "            # Find all possible prefixes excluding certain prefixes\n",
    "            all_prefixes = find_all_prefixes(melted_df.columns)\n",
    "            # Remove all identified prefixes\n",
    "            melted_df = remove_prefixes(melted_df, all_prefixes)\n",
    "\n",
    "    else:\n",
    "        df =  df_initial.copy()\n",
    "\n",
    "        # Step 1: Replace null or unnamed headers with inferred names (if needed)\n",
    "        unnamed_columns = df.columns.str.contains('^Unnamed')\n",
    "        new_columns = []\n",
    "        last_valid_header = None\n",
    "\n",
    "        for i, col in enumerate(df.columns):\n",
    "            if unnamed_columns[i]:\n",
    "                if last_valid_header is not None:\n",
    "                    new_columns.append(last_valid_header)\n",
    "                else:\n",
    "                    new_columns.append('Unnamed')\n",
    "            else:\n",
    "                new_columns.append(col)\n",
    "                last_valid_header = col\n",
    "\n",
    "        df.columns = new_columns\n",
    "\n",
    "        # Step 2: Drop rows with all NaN values as well as column also with complete NaN value\n",
    "        df = df.dropna(how='all').reset_index(drop=True) # for row drop\n",
    "        df = df.dropna(axis=1, how='all') # for Column drop\n",
    "\n",
    "        # # Step 2: Initialize a list to store notes\n",
    "        # notes = []\n",
    "        # #print(notes)\n",
    "\n",
    "        # # Step 3: Iterate through each row in the DataFrame to separate notes and remove them from df\n",
    "        # index_to_drop = []  # To store indices of rows to drop from df\n",
    "        # # for index, row in df.iterrows():\n",
    "        # #     if pd.notna(row[0]) and not any(row.dropna().index[1:]):  # Check if it's a note row\n",
    "        # #         notes.append(row[0])  # Append note to notes list\n",
    "        # #         index_to_drop.append(index)  # Mark row index to drop\n",
    "\n",
    "        # for index, row in df.iterrows():\n",
    "        #     if (pd.notna(row[0]) or pd.notna(row[1])) and not any(row.dropna().index[2:]):  # Check if it's a note row\n",
    "        #         notes.append(row[0] if pd.notna(row[0]) else row[1])  # Append note to notes list\n",
    "        #         index_to_drop.append(index) \n",
    "\n",
    "        # # Step 4: Keep rows up to and including the smallest index in index_to_drop\n",
    "        # if index_to_drop:\n",
    "        #     min_index = min(index_to_drop)  # Find the smallest index to keep\n",
    "        #     df = df.iloc[:min_index].reset_index(drop=True)  # Keep rows from the beginning up to but not including this index\n",
    "\n",
    "        # Step 3: Find the first numerical row\n",
    "        first_num_row_idx = find_first_numerical_row(df)\n",
    "\n",
    "        # Process only if the first numerical row is not the first row of the DataFrame\n",
    "        if first_num_row_idx > 0:\n",
    "            # Step 4: Extract original headers and rows above the first numerical row\n",
    "            original_headers = df.columns.tolist()\n",
    "            above_rows = df.iloc[:first_num_row_idx]\n",
    "\n",
    "            # to fill the row horizontaly to make proper header    \n",
    "            above_rows_filled = above_rows.fillna(method='ffill', axis=1)\n",
    "\n",
    "            # assigning the value find in above step to make proper header\n",
    "            df.iloc[:first_num_row_idx] = above_rows_filled\n",
    "\n",
    "            # Collect all values from above rows into a single list\n",
    "            above_rows_values = above_rows.values.flatten().tolist()\n",
    "\n",
    "            # Create combined headers by appending values above to the original headers\n",
    "            # combined_headers = [f\"{str(header)}_{str(value)}\" if not pd.isna(value) else header for header, value in zip(original_headers, above_rows_values)]\n",
    "\n",
    "            ## the Below code is useful to handle if there is more than one row in the above row to proper fill\n",
    "            combined_headers = original_headers[:]  # Start with original headers\n",
    "            for idx, row in above_rows.iterrows():\n",
    "                combined_headers = [\n",
    "                    f\"{header}__{str(value)}\" if not pd.isna(value) else header\n",
    "                    for header, value in zip(combined_headers, row)\n",
    "                ]\n",
    "\n",
    "            # Create new DataFrame with combined headers\n",
    "            new_df = pd.DataFrame(df.iloc[first_num_row_idx:].values, columns=combined_headers)\n",
    "\n",
    "            new_df = new_df.applymap(clean_text) ## for multiline text or wrap text in row or cell\n",
    "\n",
    "            # Apply the cleaning function to the column names\n",
    "            new_df.columns = [clean_text(col) for col in new_df.columns] ## removing text of multiline and wrap from heading\n",
    "\n",
    "            new_df = clean_column_headers(new_df)\n",
    "\n",
    "            # Call the function to find the first numerical column to use melted df\n",
    "            first_numerical_row_idx, columns_with_numerical_values = find_row_with_most_numerical_values(new_df)\n",
    "            # columns_with_numerical_values, all_numerical_columns = find_all_numerical_columns(new_df)\n",
    "                \n",
    "            col_idx = new_df.columns.get_loc(columns_with_numerical_values)\n",
    "            columns_before = new_df.columns[:col_idx]\n",
    "\n",
    "            new_df[columns_before] = new_df[columns_before].fillna(method='ffill')\n",
    "\n",
    "            last_numerical_row = find_last_numerical_row(new_df, columns_before)\n",
    "\n",
    "            for col in new_df.columns[col_idx:]:\n",
    "                new_df[col] = new_df[col].apply(lambda x: f\"{float(x) * 100:.2f}\" if isinstance(x, (int, float)) and str(x).startswith('0.') else x)  #% symbol removing and value at time of read convert to decimal reconvert to same\n",
    "\n",
    "\n",
    "            # new_df = new_df.applymap(clean_text) ## for multiline text or wrap text in row or cell\n",
    "\n",
    "            #     # Apply the cleaning function to the column names\n",
    "            # new_df.columns = [clean_text(col) for col in new_df.columns] ## removing text of multiline and wrap from heading\n",
    "\n",
    "                # Loop through the columns and rename if they start with 'All_Broker_'\n",
    "            new_columns = {col: col.split('__')[0] for col in df.columns if col.lower().startswith('all broker -')}\n",
    "\n",
    "            # Rename the columns in the DataFrame\n",
    "            new_df.rename(columns=new_columns, inplace=True)\n",
    "\n",
    "                # Melt the DataFrame\n",
    "            value_vars = new_df.columns[col_idx:]\n",
    "            melted_df = pd.melt(new_df, id_vars=list(columns_before), value_vars=value_vars, var_name='variable', value_name='commission')\n",
    "\n",
    "                # Split the 'variable' column based on '/' and '_' and expand into separate columns\n",
    "            # expanded_cols = melted_df['variable'].str.split(r'[_]', expand=True)\n",
    "            expanded_cols = melted_df['variable'].str.split(r'__', expand=True)\n",
    "                \n",
    "                # Rename columns based on the maximum number of splits\n",
    "            expanded_cols.columns = [f'variable_{i+1}' for i in range(expanded_cols.shape[1])]\n",
    "                \n",
    "                # Concatenate the expanded columns to the melted DataFrame\n",
    "            melted_df = pd.concat([melted_df, expanded_cols], axis=1)\n",
    "                \n",
    "                # Rearrange columns: expanded columns after id_vars and before 'commission'\n",
    "            id_vars = list(columns_before)\n",
    "            value_name = 'commission'\n",
    "                \n",
    "            columns_order = id_vars + [col for col in expanded_cols.columns] + [value_name]\n",
    "            melted_df = melted_df[columns_order]\n",
    "\n",
    "                        # Remove rows where the commission value is a substring of any value in the id_vars columns\n",
    "            rows_to_drop = melted_df.apply(\n",
    "                lambda row: any(str(row[value_name]) in str(row[id_var]) for id_var in id_vars), axis=1)\n",
    "\n",
    "            melted_df = melted_df[~rows_to_drop].reset_index(drop=True)\n",
    "\n",
    "            melted_df.columns = [col.split('__')[-1] if col.lower().startswith('all broker') else col for col in melted_df.columns]\n",
    "            melted_df= melted_df.dropna(subset=['commission']).reset_index(drop=True)   \n",
    "            \n",
    "            melted_df.columns = rename_columns(melted_df)\n",
    "            # Apply the function to the entire DataFrame\n",
    "            melted_df = melted_df.applymap(replace_brackets)\n",
    "\n",
    "            # Apply the function to the entire DataFrame\n",
    "            melted_df = melted_df.applymap(strip_spaces)\n",
    "\n",
    "            # Find all possible prefixes excluding certain prefixes\n",
    "            all_prefixes = find_all_prefixes(melted_df.columns)\n",
    "            # Remove all identified prefixes\n",
    "            melted_df = remove_prefixes(melted_df, all_prefixes)\n",
    "\n",
    "        else:\n",
    "            # Step 2: Drop rows with all NaN values\n",
    "            df = df.dropna(how='all').reset_index(drop=True)\n",
    "\n",
    "            # Drop columns where all values are NaN\n",
    "            df = df.dropna(axis=1, how='all')\n",
    "\n",
    "            # # Step 2: Initialize a list to store notes\n",
    "            # notes = []\n",
    "            # #print(notes)\n",
    "\n",
    "            # # Step 3: Iterate through each row in the DataFrame to separate notes and remove them from df\n",
    "            # index_to_drop = []  # To store indices of rows to drop from df\n",
    "            # # for index, row in df.iterrows():\n",
    "            # #     if pd.notna(row[0]) and not any(row.dropna().index[1:]):  # Check if it's a note row\n",
    "            # #         notes.append(row[0])  # Append note to notes list\n",
    "            # #         index_to_drop.append(index)  # Mark row index to drop\n",
    "\n",
    "\n",
    "            # for index, row in df.iterrows():\n",
    "            #     if (pd.notna(row[0]) or pd.notna(row[1])) and not any(row.dropna().index[2:]):  # Check if it's a note row\n",
    "            #         notes.append(row[0] if pd.notna(row[0]) else row[1])  # Append note to notes list\n",
    "            #         index_to_drop.append(index) \n",
    "\n",
    "            # # Step 4: Keep rows up to and including the smallest index in index_to_drop\n",
    "            # if index_to_drop:\n",
    "            #     min_index = min(index_to_drop)  # Find the smallest index to keep\n",
    "            #     df = df.iloc[:min_index].reset_index(drop=True)  # Keep rows from the beginning up to but not including this index\n",
    "\n",
    "            new_df = df.copy()\n",
    "\n",
    "            new_df = new_df.applymap(clean_text) ## for multiline text or wrap text in row or cell\n",
    "\n",
    "            # Apply the cleaning function to the column names\n",
    "            new_df.columns = [clean_text(col) for col in new_df.columns] ## removing text of multiline and wrap from heading\n",
    "\n",
    "            new_df = clean_column_headers(new_df)\n",
    "\n",
    "            # Call the function\n",
    "            first_numerical_row_idx, columns_with_numerical_values = find_row_with_most_numerical_values(new_df)\n",
    "            # columns_with_numerical_values, all_numerical_columns = find_all_numerical_columns(new_df)\n",
    "            \n",
    "            col_idx = new_df.columns.get_loc(columns_with_numerical_values)\n",
    "            columns_before = new_df.columns[:col_idx]\n",
    "\n",
    "            new_df[columns_before] = new_df[columns_before].fillna(method='ffill')\n",
    "\n",
    "            last_numerical_row = find_last_numerical_row(new_df, columns_before)\n",
    "\n",
    "            for col in new_df.columns[col_idx:]:\n",
    "                new_df[col] = new_df[col].apply(lambda x: f\"{float(x) * 100:.2f}\" if isinstance(x, (int, float)) and str(x).startswith('0.') else x)  #% symbol removing and value at time of read convert to decimal reconvert to same\n",
    "\n",
    "\n",
    "            # new_df = new_df.applymap(clean_text) ## for multiline text or wrap text in row or cell\n",
    "\n",
    "            # # Apply the cleaning function to the column names\n",
    "            # new_df.columns = [clean_text(col) for col in new_df.columns] ## removing text of multiline and wrap from heading\n",
    "\n",
    "            # Loop through the columns and rename if they start with 'All_Broker_'\n",
    "            new_columns = {col: col.split('__')[0] for col in df.columns if col.lower().startswith('all broker -')}\n",
    "\n",
    "\n",
    "            # Rename the columns in the DataFrame\n",
    "            new_df.rename(columns=new_columns, inplace=True)\n",
    "\n",
    "            # Melt the DataFrame\n",
    "            value_vars = new_df.columns[col_idx:]\n",
    "            melted_df = pd.melt(new_df, id_vars=list(columns_before), value_vars=value_vars, var_name='variable', value_name='commission')\n",
    "\n",
    "            # Split the 'variable' column based on '/' and '_' and expand into separate columns\n",
    "            # expanded_cols = melted_df['variable'].str.split(r'[_]', expand=True)\n",
    "            expanded_cols = melted_df['variable'].str.split(r'__', expand=True)\n",
    "            \n",
    "            # Rename columns based on the maximum number of splits\n",
    "            expanded_cols.columns = [f'variable_{i+1}' for i in range(expanded_cols.shape[1])]\n",
    "            \n",
    "            # Concatenate the expanded columns to the melted DataFrame\n",
    "            melted_df = pd.concat([melted_df, expanded_cols], axis=1)\n",
    "            \n",
    "            # Rearrange columns: expanded columns after id_vars and before 'commission'\n",
    "            id_vars = list(columns_before)\n",
    "            value_name = 'commission'\n",
    "            \n",
    "            columns_order = id_vars + [col for col in expanded_cols.columns] + [value_name]\n",
    "            melted_df = melted_df[columns_order]\n",
    "\n",
    "                        # Remove rows where the commission value is a substring of any value in the id_vars columns\n",
    "            rows_to_drop = melted_df.apply(\n",
    "                lambda row: any(str(row[value_name]) in str(row[id_var]) for id_var in id_vars), axis=1)\n",
    "\n",
    "            melted_df = melted_df[~rows_to_drop].reset_index(drop=True)\n",
    "\n",
    "            melted_df.columns = [col.split('__')[-1] if col.lower().startswith('all broker') else col for col in melted_df.columns]\n",
    "\n",
    "            melted_df= melted_df.dropna(subset=['commission']).reset_index(drop=True)\n",
    "\n",
    "            melted_df.columns = rename_columns(melted_df)\n",
    "\n",
    "            # Apply the function to the entire DataFrame\n",
    "            melted_df = melted_df.applymap(replace_brackets)\n",
    "\n",
    "            # Apply the function to the entire DataFrame\n",
    "            melted_df = melted_df.applymap(strip_spaces)\n",
    "\n",
    "            # Find all possible prefixes excluding certain prefixes\n",
    "            all_prefixes = find_all_prefixes(melted_df.columns)\n",
    "            # Remove all identified prefixes\n",
    "            melted_df = remove_prefixes(melted_df, all_prefixes)\n",
    "                        \n",
    "\n",
    "else:\n",
    "    # Initial headers are valid, use the initially read DataFrame with headers as the first row\n",
    "    df = df_initial.copy()\n",
    "    #Step 2: Drop rows with all NaN values\n",
    "    df = df.dropna(how='all').reset_index(drop=True)\n",
    "\n",
    "    # Drop columns where all values are NaN\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "\n",
    "    # # Step 2: Initialize a list to store notes\n",
    "    # notes = []\n",
    "    # #print(notes)\n",
    "\n",
    "    # # Step 3: Iterate through each row in the DataFrame to separate notes and remove them from df\n",
    "    # index_to_drop = []  # To store indices of rows to drop from df\n",
    "    # # for index, row in df.iterrows():\n",
    "    # #     if pd.notna(row[0]) and not any(row.dropna().index[1:]):  # Check if it's a note row\n",
    "    # #         notes.append(row[0])  # Append note to notes list\n",
    "    # #         index_to_drop.append(index)  # Mark row index to drop\n",
    "\n",
    "    # for index, row in df.iterrows():\n",
    "    #     if (pd.notna(row[0]) or pd.notna(row[1])) and not any(row.dropna().index[2:]):  # Check if it's a note row\n",
    "    #         notes.append(row[0] if pd.notna(row[0]) else row[1])  # Append note to notes list\n",
    "    #         index_to_drop.append(index) \n",
    "\n",
    "    #     # Step 4: Keep rows up to and including the smallest index in index_to_drop\n",
    "    # if index_to_drop:\n",
    "    #     min_index = min(index_to_drop)  # Find the smallest index to keep\n",
    "    #     df = df.iloc[:min_index].reset_index(drop=True)  # Keep rows from the beginning up to but not including this index\n",
    "    \n",
    "    new_df = df.copy()\n",
    "\n",
    "\n",
    "    new_df = new_df.applymap(clean_text) ## for multiline text or wrap text in row or cell\n",
    "\n",
    "            # Apply the cleaning function to the column names\n",
    "    new_df.columns = [clean_text(col) for col in new_df.columns] ## removing text of multiline and wrap from heading\n",
    "\n",
    "    new_df = clean_column_headers(new_df)\n",
    "\n",
    "    # Call the function\n",
    "    first_numerical_row_idx, columns_with_numerical_values = find_row_with_most_numerical_values(new_df)\n",
    "    # columns_with_numerical_values, all_numerical_columns = find_all_numerical_columns(new_df)\n",
    "            \n",
    "    col_idx = new_df.columns.get_loc(columns_with_numerical_values)\n",
    "    columns_before = new_df.columns[:col_idx]\n",
    "\n",
    "    new_df[columns_before] = new_df[columns_before].fillna(method='ffill')\n",
    "\n",
    "    last_numerical_row = find_last_numerical_row(new_df, columns_before)\n",
    "\n",
    "\n",
    "\n",
    "    for col in new_df.columns[col_idx:]:\n",
    "        new_df[col] = new_df[col].apply(lambda x: f\"{float(x) * 100:.2f}\" if isinstance(x, (int, float)) and str(x).startswith('0.') else x)  #% symbol removing and value at time of read convert to decimal reconvert to same\n",
    "\n",
    "\n",
    "    # new_df = new_df.applymap(clean_text) ## for multiline text or wrap text in row or cell\n",
    "\n",
    "    # #         # Apply the cleaning function to the column names\n",
    "    # new_df.columns = [clean_text(col) for col in new_df.columns] ## removing text of multiline and wrap from heading\n",
    "\n",
    "    # Loop through the columns and rename if they start with 'All_Broker_'\n",
    "    new_columns = {col: col.split('__')[0] for col in df.columns if col.lower().startswith('all broker -')}\n",
    "\n",
    "    # Rename the columns in the DataFrame\n",
    "    new_df.rename(columns=new_columns, inplace=True)\n",
    "\n",
    "    # melted_df= new_df.dropna(subset=[columns_with_numerical_values]).reset_index(drop=True)\n",
    "    melted_df = new_df.copy()\n",
    "\n",
    "    # Define a more flexible pattern for extracting details\n",
    "    pattern = re.compile(r'Diesel upto (\\d+)cc \\(all model will cover\\)\\s*(?:and|&)?\\s*all cars above (\\d+)cc\\s*(?:except|excluding)\\s*(\\w+)',re.IGNORECASE)\n",
    "\n",
    "    # Initialize variables for extracted details\n",
    "    diesel_cc, large_cars_cc, except_model = None, None, None\n",
    "\n",
    "    # Search the entire DataFrame for the pattern in cell values\n",
    "    for cell in melted_df.values.flatten():\n",
    "        cell_str = str(cell)  # Convert cell to string for pattern matching\n",
    "        match = pattern.search(cell_str)\n",
    "        if match:\n",
    "            diesel_cc, large_cars_cc, except_model = match.groups()\n",
    "            # print(f\"Found details - diesel_cc: {diesel_cc}, large_cars_cc: {large_cars_cc}, except_model: {except_model}\")\n",
    "            break\n",
    "\n",
    "    # Define the replacement text if required values were found\n",
    "    if diesel_cc and large_cars_cc and except_model:\n",
    "        replacement_text = (f'All other Cars - Petrol upto {large_cars_cc}cc and Diesel above {diesel_cc}cc. Above {large_cars_cc}cc For {except_model}')\n",
    "        # Replace the text across the entire DataFrame\n",
    "        melted_df.replace('All other Cars except above model', replacement_text, inplace=True)\n",
    "\n",
    "    # Define the substring to replace and the replacement value\n",
    "    # substring_to_replace = \n",
    "    if except_model == 'TTMMHHK':\n",
    "        replacement_value = 'TATA|TOYOTA|MARUTI|MAHINDRA|HONDA|HYUNDAI|KIA'\n",
    "\n",
    "        # Replace the substring in the entire DataFrame\n",
    "        melted_df = melted_df.applymap(lambda x: x.replace(except_model, replacement_value) if isinstance(x, str) else x)\n",
    "    else:\n",
    "        melted_df\n",
    "\n",
    "    if columns_with_numerical_values == 'Renewal' or columns_with_numerical_values == 'commission':\n",
    "        melted_df= melted_df.dropna(subset=[columns_with_numerical_values]).reset_index(drop=True)\n",
    "        melted_df = melted_df.drop(columns= ['Comm on Net Premium of 1st year', 'Unnamed: 5'])\n",
    "\n",
    "    else:\n",
    "        melted_df\n",
    "\n",
    "\n",
    "    melted_df.columns = rename_columns(melted_df)\n",
    "\n",
    "    # Apply the function to the entire DataFrame\n",
    "    melted_df = melted_df.applymap(replace_brackets)\n",
    "\n",
    "    # # Remove rows where the commission value is a substring of any value in the id_vars columns\n",
    "    # rows_to_drop = melted_df.apply(\n",
    "    #     lambda row: any(str(row[value_name]) in str(row[id_var]) for id_var in id_vars), axis=1)\n",
    "\n",
    "    # melted_df = melted_df[~rows_to_drop].reset_index(drop=True)\n",
    "\n",
    "    # Apply the function to the entire DataFrame\n",
    "    melted_df = melted_df.applymap(strip_spaces)\n",
    "\n",
    "    # Find all possible prefixes excluding certain prefixes\n",
    "    all_prefixes = find_all_prefixes(melted_df.columns)\n",
    "    # Remove all identified prefixes\n",
    "    melted_df = remove_prefixes(melted_df, all_prefixes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RTO CATEGORY</th>\n",
       "      <th>RTO Zone</th>\n",
       "      <th>RTO State</th>\n",
       "      <th>RTO Location</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "      <th>Two Wheeler</th>\n",
       "      <th>Unnamed: 13</th>\n",
       "      <th>Unnamed: 14</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "      <th>Unnamed: 17</th>\n",
       "      <th>Unnamed: 18</th>\n",
       "      <th>Unnamed: 19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Private car- New, SAOD,Comp and Used Car      ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Private car AOTP  (Points on Net)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Scooters  (Points on Net)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bikes (Points on net)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EV-TW (Registered &amp; Non Registered)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pvt Car New 1+3</td>\n",
       "      <td>Pvt Car Petrol &amp; CNG- 1+1 (NCB Cases)</td>\n",
       "      <td>Pvt Car Diesel &amp; EV - 1+1 (NCB Cases)</td>\n",
       "      <td>SAOD-NCB</td>\n",
       "      <td>Pvt Car-\\n0 NCB ( NON NCB)</td>\n",
       "      <td>Pvt Car (Used Car**)</td>\n",
       "      <td>Pvt car AOTP- Petrol</td>\n",
       "      <td>Pvt car AOTP- Diesel</td>\n",
       "      <td>TW Scooters- Comp\\n (1+1) (2+2) (3+3)</td>\n",
       "      <td>TW Scooters - (TP only)\\n(0+1) (0+2) (0+3)</td>\n",
       "      <td>TW Scooters- SAOD</td>\n",
       "      <td>TW Bikes Comp (1+1) (2+2) (3+3) (Excluding Baj...</td>\n",
       "      <td>TW Bikes- AOTP \\n(All Manufacturers) \\n(0+1, 0...</td>\n",
       "      <td>TW Bikes -SAOD \\n(All Manufacturers)</td>\n",
       "      <td>Royal Enfield- (SAOD/Comp/ AOTP)</td>\n",
       "      <td>TW - EV (SAOD/Comp/AOTP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Geo</td>\n",
       "      <td>South</td>\n",
       "      <td>ANDHRA PRADESH</td>\n",
       "      <td>Rest of Andhra Pradesh</td>\n",
       "      <td>0.25</td>\n",
       "      <td>25% Maruti and Hyundai, Rest 15%</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.35</td>\n",
       "      <td>20% HMC</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  RTO CATEGORY RTO Zone       RTO State            RTO Location  \\\n",
       "0          NaN      NaN             NaN                     NaN   \n",
       "1          NaN      NaN             NaN                     NaN   \n",
       "2          Geo    South  ANDHRA PRADESH  Rest of Andhra Pradesh   \n",
       "\n",
       "                                          Unnamed: 4  \\\n",
       "0  Private car- New, SAOD,Comp and Used Car      ...   \n",
       "1                                   Pvt Car New 1+3    \n",
       "2                                               0.25   \n",
       "\n",
       "                              Unnamed: 5  \\\n",
       "0                                    NaN   \n",
       "1  Pvt Car Petrol & CNG- 1+1 (NCB Cases)   \n",
       "2       25% Maruti and Hyundai, Rest 15%   \n",
       "\n",
       "                              Unnamed: 6 Unnamed: 7  \\\n",
       "0                                    NaN        NaN   \n",
       "1  Pvt Car Diesel & EV - 1+1 (NCB Cases)   SAOD-NCB   \n",
       "2                                   0.15      0.225   \n",
       "\n",
       "                   Unnamed: 8            Unnamed: 9  \\\n",
       "0                         NaN                   NaN   \n",
       "1  Pvt Car-\\n0 NCB ( NON NCB)  Pvt Car (Used Car**)   \n",
       "2                        0.15                  0.15   \n",
       "\n",
       "                         Unnamed: 10           Unnamed: 11  \\\n",
       "0  Private car AOTP  (Points on Net)                   NaN   \n",
       "1               Pvt car AOTP- Petrol  Pvt car AOTP- Diesel   \n",
       "2                               0.25                  0.05   \n",
       "\n",
       "                             Two Wheeler  \\\n",
       "0              Scooters  (Points on Net)   \n",
       "1  TW Scooters- Comp\\n (1+1) (2+2) (3+3)   \n",
       "2                                    0.4   \n",
       "\n",
       "                                  Unnamed: 13        Unnamed: 14  \\\n",
       "0                                         NaN                NaN   \n",
       "1  TW Scooters - (TP only)\\n(0+1) (0+2) (0+3)  TW Scooters- SAOD   \n",
       "2                                         0.1               0.35   \n",
       "\n",
       "                                         Unnamed: 15  \\\n",
       "0                              Bikes (Points on net)   \n",
       "1  TW Bikes Comp (1+1) (2+2) (3+3) (Excluding Baj...   \n",
       "2                                            20% HMC   \n",
       "\n",
       "                                         Unnamed: 16  \\\n",
       "0                                                NaN   \n",
       "1  TW Bikes- AOTP \\n(All Manufacturers) \\n(0+1, 0...   \n",
       "2                                               0.15   \n",
       "\n",
       "                            Unnamed: 17                        Unnamed: 18  \\\n",
       "0                                   NaN                                NaN   \n",
       "1  TW Bikes -SAOD \\n(All Manufacturers)  Royal Enfield- (SAOD/Comp/ AOTP)    \n",
       "2                                   0.3                                  0   \n",
       "\n",
       "                           Unnamed: 19  \n",
       "0  EV-TW (Registered & Non Registered)  \n",
       "1             TW - EV (SAOD/Comp/AOTP)  \n",
       "2                                  0.3  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_initial.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RTO CATEGORY</th>\n",
       "      <th>RTO Zone</th>\n",
       "      <th>RTO State</th>\n",
       "      <th>RTO Location</th>\n",
       "      <th>RTO Location</th>\n",
       "      <th>RTO Location</th>\n",
       "      <th>RTO Location</th>\n",
       "      <th>RTO Location</th>\n",
       "      <th>RTO Location</th>\n",
       "      <th>RTO Location</th>\n",
       "      <th>RTO Location</th>\n",
       "      <th>RTO Location</th>\n",
       "      <th>Two Wheeler</th>\n",
       "      <th>Two Wheeler</th>\n",
       "      <th>Two Wheeler</th>\n",
       "      <th>Two Wheeler</th>\n",
       "      <th>Two Wheeler</th>\n",
       "      <th>Two Wheeler</th>\n",
       "      <th>Two Wheeler</th>\n",
       "      <th>Two Wheeler</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Private car- New, SAOD,Comp and Used Car      ...</td>\n",
       "      <td>Private car- New, SAOD,Comp and Used Car      ...</td>\n",
       "      <td>Private car- New, SAOD,Comp and Used Car      ...</td>\n",
       "      <td>Private car- New, SAOD,Comp and Used Car      ...</td>\n",
       "      <td>Private car- New, SAOD,Comp and Used Car      ...</td>\n",
       "      <td>Private car- New, SAOD,Comp and Used Car      ...</td>\n",
       "      <td>Private car AOTP  (Points on Net)</td>\n",
       "      <td>Private car AOTP  (Points on Net)</td>\n",
       "      <td>Scooters  (Points on Net)</td>\n",
       "      <td>Scooters  (Points on Net)</td>\n",
       "      <td>Scooters  (Points on Net)</td>\n",
       "      <td>Bikes (Points on net)</td>\n",
       "      <td>Bikes (Points on net)</td>\n",
       "      <td>Bikes (Points on net)</td>\n",
       "      <td>Bikes (Points on net)</td>\n",
       "      <td>EV-TW (Registered &amp; Non Registered)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pvt Car New 1+3</td>\n",
       "      <td>Pvt Car Petrol &amp; CNG- 1+1 (NCB Cases)</td>\n",
       "      <td>Pvt Car Diesel &amp; EV - 1+1 (NCB Cases)</td>\n",
       "      <td>SAOD-NCB</td>\n",
       "      <td>Pvt Car-\\n0 NCB ( NON NCB)</td>\n",
       "      <td>Pvt Car (Used Car**)</td>\n",
       "      <td>Pvt car AOTP- Petrol</td>\n",
       "      <td>Pvt car AOTP- Diesel</td>\n",
       "      <td>TW Scooters- Comp\\n (1+1) (2+2) (3+3)</td>\n",
       "      <td>TW Scooters - (TP only)\\n(0+1) (0+2) (0+3)</td>\n",
       "      <td>TW Scooters- SAOD</td>\n",
       "      <td>TW Bikes Comp (1+1) (2+2) (3+3) (Excluding Baj...</td>\n",
       "      <td>TW Bikes- AOTP \\n(All Manufacturers) \\n(0+1, 0...</td>\n",
       "      <td>TW Bikes -SAOD \\n(All Manufacturers)</td>\n",
       "      <td>Royal Enfield- (SAOD/Comp/ AOTP)</td>\n",
       "      <td>TW - EV (SAOD/Comp/AOTP)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  RTO CATEGORY RTO Zone RTO State RTO Location  \\\n",
       "0          NaN      NaN       NaN          NaN   \n",
       "1          NaN      NaN       NaN          NaN   \n",
       "\n",
       "                                        RTO Location  \\\n",
       "0  Private car- New, SAOD,Comp and Used Car      ...   \n",
       "1                                   Pvt Car New 1+3    \n",
       "\n",
       "                                        RTO Location  \\\n",
       "0  Private car- New, SAOD,Comp and Used Car      ...   \n",
       "1              Pvt Car Petrol & CNG- 1+1 (NCB Cases)   \n",
       "\n",
       "                                        RTO Location  \\\n",
       "0  Private car- New, SAOD,Comp and Used Car      ...   \n",
       "1              Pvt Car Diesel & EV - 1+1 (NCB Cases)   \n",
       "\n",
       "                                        RTO Location  \\\n",
       "0  Private car- New, SAOD,Comp and Used Car      ...   \n",
       "1                                           SAOD-NCB   \n",
       "\n",
       "                                        RTO Location  \\\n",
       "0  Private car- New, SAOD,Comp and Used Car      ...   \n",
       "1                         Pvt Car-\\n0 NCB ( NON NCB)   \n",
       "\n",
       "                                        RTO Location  \\\n",
       "0  Private car- New, SAOD,Comp and Used Car      ...   \n",
       "1                               Pvt Car (Used Car**)   \n",
       "\n",
       "                        RTO Location                       RTO Location  \\\n",
       "0  Private car AOTP  (Points on Net)  Private car AOTP  (Points on Net)   \n",
       "1               Pvt car AOTP- Petrol               Pvt car AOTP- Diesel   \n",
       "\n",
       "                             Two Wheeler  \\\n",
       "0              Scooters  (Points on Net)   \n",
       "1  TW Scooters- Comp\\n (1+1) (2+2) (3+3)   \n",
       "\n",
       "                                  Two Wheeler                Two Wheeler  \\\n",
       "0                   Scooters  (Points on Net)  Scooters  (Points on Net)   \n",
       "1  TW Scooters - (TP only)\\n(0+1) (0+2) (0+3)          TW Scooters- SAOD   \n",
       "\n",
       "                                         Two Wheeler  \\\n",
       "0                              Bikes (Points on net)   \n",
       "1  TW Bikes Comp (1+1) (2+2) (3+3) (Excluding Baj...   \n",
       "\n",
       "                                         Two Wheeler  \\\n",
       "0                              Bikes (Points on net)   \n",
       "1  TW Bikes- AOTP \\n(All Manufacturers) \\n(0+1, 0...   \n",
       "\n",
       "                            Two Wheeler                        Two Wheeler  \\\n",
       "0                 Bikes (Points on net)              Bikes (Points on net)   \n",
       "1  TW Bikes -SAOD \\n(All Manufacturers)  Royal Enfield- (SAOD/Comp/ AOTP)    \n",
       "\n",
       "                           Two Wheeler  \n",
       "0  EV-TW (Registered & Non Registered)  \n",
       "1             TW - EV (SAOD/Comp/AOTP)  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RTO CATEGORY</th>\n",
       "      <th>RTO Zone</th>\n",
       "      <th>RTO State</th>\n",
       "      <th>RTO Location</th>\n",
       "      <th>RTO Location__Private car- New, SAOD,Comp and Used Car (Points on OD Prem*)__Pvt Car New 1+3</th>\n",
       "      <th>RTO Location__Private car- New, SAOD,Comp and Used Car (Points on OD Prem*)__Pvt Car Petrol &amp; CNG- 1+1 (NCB Cases)</th>\n",
       "      <th>RTO Location__Private car- New, SAOD,Comp and Used Car (Points on OD Prem*)__Pvt Car Diesel &amp; EV - 1+1 (NCB Cases)</th>\n",
       "      <th>RTO Location__Private car- New, SAOD,Comp and Used Car (Points on OD Prem*)__SAOD-NCB</th>\n",
       "      <th>RTO Location__Private car- New, SAOD,Comp and Used Car (Points on OD Prem*)__Pvt Car- 0 NCB ( NON NCB)</th>\n",
       "      <th>RTO Location__Private car- New, SAOD,Comp and Used Car (Points on OD Prem*)__Pvt Car (Used Car**)</th>\n",
       "      <th>RTO Location__Private car AOTP (Points on Net)__Pvt car AOTP- Petrol</th>\n",
       "      <th>RTO Location__Private car AOTP (Points on Net)__Pvt car AOTP- Diesel</th>\n",
       "      <th>Two Wheeler__Scooters (Points on Net)__TW Scooters- Comp (1+1) (2+2) (3+3)</th>\n",
       "      <th>Two Wheeler__Scooters (Points on Net)__TW Scooters - (TP only) (0+1) (0+2) (0+3)</th>\n",
       "      <th>Two Wheeler__Scooters (Points on Net)__TW Scooters- SAOD</th>\n",
       "      <th>Two Wheeler__Bikes (Points on net)__TW Bikes Comp (1+1) (2+2) (3+3) (Excluding Bajaj, Yamaha, TVS and Suzuki)</th>\n",
       "      <th>Two Wheeler__Bikes (Points on net)__TW Bikes- AOTP (All Manufacturers) (0+1, 0+2, 0+3)</th>\n",
       "      <th>Two Wheeler__Bikes (Points on net)__TW Bikes -SAOD (All Manufacturers)</th>\n",
       "      <th>Two Wheeler__Bikes (Points on net)__Royal Enfield- (SAOD/Comp/ AOTP)</th>\n",
       "      <th>Two Wheeler__EV-TW (Registered &amp; Non Registered)__TW - EV (SAOD/Comp/AOTP)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Geo</td>\n",
       "      <td>South</td>\n",
       "      <td>ANDHRA PRADESH</td>\n",
       "      <td>Rest of Andhra Pradesh</td>\n",
       "      <td>25.00</td>\n",
       "      <td>25% Maruti and Hyundai, Rest 15%</td>\n",
       "      <td>15.00</td>\n",
       "      <td>22.50</td>\n",
       "      <td>15.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>35.00</td>\n",
       "      <td>20% HMC</td>\n",
       "      <td>15.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vertical</td>\n",
       "      <td>South</td>\n",
       "      <td>ANDHRA PRADESH</td>\n",
       "      <td>Vijaywada</td>\n",
       "      <td>25.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>45.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>35.00</td>\n",
       "      <td>25% HMC, 40% HMSI</td>\n",
       "      <td>0</td>\n",
       "      <td>30.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  RTO CATEGORY RTO Zone       RTO State            RTO Location  \\\n",
       "0          Geo    South  ANDHRA PRADESH  Rest of Andhra Pradesh   \n",
       "1     Vertical    South  ANDHRA PRADESH               Vijaywada   \n",
       "\n",
       "  RTO Location__Private car- New, SAOD,Comp and Used Car (Points on OD Prem*)__Pvt Car New 1+3  \\\n",
       "0                                              25.00                                             \n",
       "1                                              25.00                                             \n",
       "\n",
       "  RTO Location__Private car- New, SAOD,Comp and Used Car (Points on OD Prem*)__Pvt Car Petrol & CNG- 1+1 (NCB Cases)  \\\n",
       "0                   25% Maruti and Hyundai, Rest 15%                                                                   \n",
       "1                                              25.00                                                                   \n",
       "\n",
       "  RTO Location__Private car- New, SAOD,Comp and Used Car (Points on OD Prem*)__Pvt Car Diesel & EV - 1+1 (NCB Cases)  \\\n",
       "0                                              15.00                                                                   \n",
       "1                                              25.00                                                                   \n",
       "\n",
       "  RTO Location__Private car- New, SAOD,Comp and Used Car (Points on OD Prem*)__SAOD-NCB  \\\n",
       "0                                              22.50                                      \n",
       "1                                              25.00                                      \n",
       "\n",
       "  RTO Location__Private car- New, SAOD,Comp and Used Car (Points on OD Prem*)__Pvt Car- 0 NCB ( NON NCB)  \\\n",
       "0                                              15.00                                                       \n",
       "1                                              15.00                                                       \n",
       "\n",
       "  RTO Location__Private car- New, SAOD,Comp and Used Car (Points on OD Prem*)__Pvt Car (Used Car**)  \\\n",
       "0                                              15.00                                                  \n",
       "1                                              25.00                                                  \n",
       "\n",
       "  RTO Location__Private car AOTP (Points on Net)__Pvt car AOTP- Petrol  \\\n",
       "0                                              25.00                     \n",
       "1                                              25.00                     \n",
       "\n",
       "  RTO Location__Private car AOTP (Points on Net)__Pvt car AOTP- Diesel  \\\n",
       "0                                               5.00                     \n",
       "1                                              15.00                     \n",
       "\n",
       "  Two Wheeler__Scooters (Points on Net)__TW Scooters- Comp (1+1) (2+2) (3+3)  \\\n",
       "0                                              40.00                           \n",
       "1                                              45.00                           \n",
       "\n",
       "  Two Wheeler__Scooters (Points on Net)__TW Scooters - (TP only) (0+1) (0+2) (0+3)  \\\n",
       "0                                              10.00                                 \n",
       "1                                              40.00                                 \n",
       "\n",
       "  Two Wheeler__Scooters (Points on Net)__TW Scooters- SAOD  \\\n",
       "0                                              35.00         \n",
       "1                                              35.00         \n",
       "\n",
       "  Two Wheeler__Bikes (Points on net)__TW Bikes Comp (1+1) (2+2) (3+3) (Excluding Bajaj, Yamaha, TVS and Suzuki)  \\\n",
       "0                                            20% HMC                                                              \n",
       "1                                  25% HMC, 40% HMSI                                                              \n",
       "\n",
       "  Two Wheeler__Bikes (Points on net)__TW Bikes- AOTP (All Manufacturers) (0+1, 0+2, 0+3)  \\\n",
       "0                                              15.00                                       \n",
       "1                                                  0                                       \n",
       "\n",
       "  Two Wheeler__Bikes (Points on net)__TW Bikes -SAOD (All Manufacturers)  \\\n",
       "0                                              30.00                       \n",
       "1                                              30.00                       \n",
       "\n",
       "  Two Wheeler__Bikes (Points on net)__Royal Enfield- (SAOD/Comp/ AOTP)  \\\n",
       "0                                               0.00                     \n",
       "1                                              40.00                     \n",
       "\n",
       "  Two Wheeler__EV-TW (Registered & Non Registered)__TW - EV (SAOD/Comp/AOTP)  \n",
       "0                                              30.00                          \n",
       "1                                              30.00                          "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RTO CATEGORY</th>\n",
       "      <th>RTO Zone</th>\n",
       "      <th>RTO State</th>\n",
       "      <th>RTO Location</th>\n",
       "      <th>variable_1</th>\n",
       "      <th>variable_2</th>\n",
       "      <th>variable_3</th>\n",
       "      <th>commission</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Geo</td>\n",
       "      <td>South</td>\n",
       "      <td>ANDHRA PRADESH</td>\n",
       "      <td>Rest of Andhra Pradesh</td>\n",
       "      <td>RTO Location</td>\n",
       "      <td>Private car- New, SAOD,Comp and Used Car_Point...</td>\n",
       "      <td>Pvt Car New 1+3</td>\n",
       "      <td>25.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vertical</td>\n",
       "      <td>South</td>\n",
       "      <td>ANDHRA PRADESH</td>\n",
       "      <td>Vijaywada</td>\n",
       "      <td>RTO Location</td>\n",
       "      <td>Private car- New, SAOD,Comp and Used Car_Point...</td>\n",
       "      <td>Pvt Car New 1+3</td>\n",
       "      <td>25.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  RTO CATEGORY RTO Zone       RTO State            RTO Location    variable_1  \\\n",
       "0          Geo    South  ANDHRA PRADESH  Rest of Andhra Pradesh  RTO Location   \n",
       "1     Vertical    South  ANDHRA PRADESH               Vijaywada  RTO Location   \n",
       "\n",
       "                                          variable_2       variable_3  \\\n",
       "0  Private car- New, SAOD,Comp and Used Car_Point...  Pvt Car New 1+3   \n",
       "1  Private car- New, SAOD,Comp and Used Car_Point...  Pvt Car New 1+3   \n",
       "\n",
       "  commission  \n",
       "0      25.00  \n",
       "1      25.00  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melted_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First numerical column: commission\n",
      "Index of the column: 7\n"
     ]
    }
   ],
   "source": [
    "# Function to find the first numerical column and its index\n",
    "def find_first_numerical_column(df):\n",
    "    for index, column in enumerate(df.columns):\n",
    "        # Try converting the column to numeric\n",
    "        numeric_column = pd.to_numeric(df[column], errors='coerce')\n",
    "        # print(numeric_column)\n",
    "        # Check if there are any non-NaN values\n",
    "        if numeric_column.dropna().size > 0:\n",
    "            if index == 0:\n",
    "                continue\n",
    "            return column, index\n",
    "    return None, None\n",
    "\n",
    "# def find_first_numerical_column(df):\n",
    "#     for index, column in enumerate(df.columns[1:], start=1):  # Start from the second column\n",
    "#         # Try converting the column to numeric\n",
    "#         numeric_column = pd.to_numeric(df[column], errors='coerce')\n",
    "#         # Check if there are any non-NaN values\n",
    "#         if numeric_column.dropna().size > 0:\n",
    "#             return column, index\n",
    "#     return None, None\n",
    "\n",
    "\n",
    "# Get the first numerical column name and index\n",
    "first_numerical_column, column_index = find_first_numerical_column(melted_df)\n",
    "print(f\"First numerical column: {first_numerical_column}\")\n",
    "print(f\"Index of the column: {column_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['RTO CATEGORY', 'RTO Zone', 'RTO State', 'RTO Location', 'variable_1',\n",
      "       'variable_2', 'variable_3'],\n",
      "      dtype='object')\n",
      "Index(['commission'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def df_to_hierarchical_json(df, num_levels):\n",
    "    \"\"\"\n",
    "    Convert a DataFrame to a hierarchical JSON format.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to convert.\n",
    "        num_levels (int): Number of hierarchical levels to extract from the start of the columns.\n",
    "\n",
    "    Returns:\n",
    "        str: JSON string of the hierarchical structure.\n",
    "    \"\"\"\n",
    "    if num_levels > 1:\n",
    "        # Ensure num_levels is not more than the number of columns\n",
    "        num_levels = min(num_levels, len(df.columns))\n",
    "        \n",
    "        # Identify hierarchical columns\n",
    "        level_columns = df.columns[:num_levels]\n",
    "        print(level_columns)\n",
    "        value_columns = df.columns[num_levels:]\n",
    "        print(value_columns)\n",
    "        \n",
    "        # Initialize the hierarchical dictionary\n",
    "        hierarchy = {}\n",
    "        \n",
    "        # Group by hierarchical levels\n",
    "        grouped = df.groupby(list(level_columns))\n",
    "        \n",
    "        # Populate the hierarchical dictionary\n",
    "        for keys, group in grouped:\n",
    "            current_level = hierarchy\n",
    "            for key in keys:\n",
    "                key = str(key)\n",
    "                if key not in current_level:\n",
    "                    current_level[key] = {}\n",
    "                current_level = current_level[key]\n",
    "            \n",
    "            # Append the values at the deepest level\n",
    "            for _, row in group.iterrows():\n",
    "                values = {col: (row[col] if not pd.isna(row[col]) else None) for col in value_columns}\n",
    "                # current_level.setdefault('values', []).append(values)\n",
    "                if 'values' not in current_level:\n",
    "                    current_level['values'] = []\n",
    "                current_level['values'].append(values)\n",
    "        \n",
    "        # Convert dictionary to JSON\n",
    "        return json.dumps(hierarchy, indent=2)\n",
    "    else:\n",
    "            # Use columns before the numerical column as hierarchical levels\n",
    "        # and columns after as value columns\n",
    "        level_columns = df.columns[:column_index]\n",
    "        value_columns = df.columns[column_index:]\n",
    "        \n",
    "        # Initialize the hierarchical dictionary\n",
    "        hierarchy = {}\n",
    "\n",
    "        # Populate the hierarchical dictionary\n",
    "        for index, row in df.iterrows():\n",
    "            current_level = hierarchy\n",
    "            # Use the entire row for hierarchical levels\n",
    "            for col in level_columns:\n",
    "                key = row[col]\n",
    "                key = str(key)\n",
    "                if key not in current_level:\n",
    "                    current_level[key] = {}\n",
    "                current_level = current_level[key]\n",
    "\n",
    "            # Append the values at the deepest level\n",
    "            values = {col: row[col] if not pd.isna(row[col]) else None for col in value_columns}\n",
    "            if 'values' not in current_level:\n",
    "                current_level['values'] = []\n",
    "            current_level['values'].append(values)\n",
    "\n",
    "        # Convert dictionary to JSON\n",
    "        return json.dumps(hierarchy, indent=2)\n",
    "\n",
    "# Prompt the user for the number of levels\n",
    "# num_levels = min(5, len(df.columns))\n",
    "\n",
    "\n",
    "# Convert to hierarchical JSON with user-defined levels\n",
    "hierarchical_json = df_to_hierarchical_json(melted_df, num_levels = column_index)\n",
    "\n",
    "# Store the hierarchical JSON into a file\n",
    "with open(f'/home/choice/Desktop/insurance-fullstack/data/JSON/{sheet_name}.json', 'w') as file:\n",
    "    file.write(hierarchical_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melted_df['State (RTO)'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melted_df['variable_1'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melted_df[(melted_df['State (RTO)']=='KA_Bangalore') & (melted_df['variable_3']== 'SCOOTER')]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
