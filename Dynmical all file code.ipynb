{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Pandas display options to show all columns and rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read the file without specifying headers\n",
    "# file_path = r'D:\\Gopal\\commission model\\Commision Python script as per shukat\\Excel file use\\Test File\\Bajaj TW.xlsx'\n",
    "# file_path = r'D:\\Gopal\\commission model\\Commision Python script as per shukat\\Excel file use\\Test File\\Chola Pvt car July24 .xlsx'\n",
    "# file_path = r'D:\\Gopal\\commission model\\Commision Python script as per shukat\\Excel file use\\Test File\\Chola Tw july 2024.xlsx'\n",
    "# file_path = r'D:\\Gopal\\commission model\\Commision Python script as per shukat\\Excel file use\\Test File\\chola cv july 24.xlsx'\n",
    "# file_path = r'D:\\Gopal\\commission model\\Commision Python script as per shukat\\Excel file use\\Test File\\Digit 4w SATP July24.xlsx'\n",
    "# file_path = r'D:\\Gopal\\commission model\\Commision Python script as per shukat\\Excel file use\\Test File\\Digit TW Comp + SATP + SAOD July 24.xlsx'\n",
    "# file_path = r'D:\\Gopal\\commission model\\Commision Python script as per shukat\\Excel file use\\Test File\\ICICI Commerical July24.xlsx'\n",
    "# file_path= r'D:\\Gopal\\commission model\\Commision Python script as per shukat\\Excel file use\\Test File\\ICICI TW new July24.xlsx'\n",
    "# file_path = r'D:\\Gopal\\commission model\\Commision Python script as per shukat\\Excel file use\\Test File\\ICICI TW&PvtCar June24.xlsx'\n",
    "file_path = r'D:\\Gopal\\commission model\\Commision Python script as per shukat\\Excel file use\\Test File\\New_India_June24.xlsx'\n",
    "# file_path = r'D:\\Gopal\\commission model\\Commision Python script as per shukat\\Excel file use\\Test File\\United before April.xlsx'\n",
    "# file_path = r'D:\\Gopal\\commission model\\Commision Python script as per shukat\\Excel file use\\Test File\\United_July24.xlsx'\n",
    "# file_path = r'D:\\Gopal\\commission model\\Commision Python script as per shukat\\Excel file use\\Test File\\sbi_july.xlsx'\n",
    "# file_path = r'D:\\Gopal\\commission model\\Commision Python script as per shukat\\Excel file use\\Test File\\SBI_June24.xlsx'\n",
    "# file_path = r'D:\\Gopal\\commission model\\Commision Python script as per shukat\\Excel file use\\Test File\\Reliance CV 4 july 2024.xlsx' \n",
    "# file_path = r'D:\\Gopal\\commission model\\Commision Python script as per shukat\\Excel file use\\Test File\\Reliance TW July 24.xlsx'\n",
    "# file_path = r'D:\\Gopal\\commission model\\Commision Python script as per shukat\\Excel file use\\Test File\\Reliance PVT TP 24 july 24.xlsx'\n",
    "# file_path = r'D:\\Gopal\\commission model\\Commision Python script as per shukat\\Excel file use\\Test File\\TATA AIG 2w.xlsx'\n",
    "# file_path = r'D:\\Gopal\\commission model\\Commision Python script as per shukat\\Excel file use\\Test File\\TATA AIG Pvtcar july 2024.xlsx'\n",
    "# file_path = r'D:\\Gopal\\commission model\\Commision Python script as per shukat\\Excel file use\\Test File\\TATA AIG CV July 24.xlsx'\n",
    "# file_path = r\"D:\\Gopal\\commission model\\Commision Python script as per shukat\\Excel file use\\Chola_TW data jan to july.xlsx\"\n",
    "# file_path = r'D:\\Gopal\\commission model\\Commision Python script as per shukat\\Excel file use\\Test File\\Digit 4W feb 24.xlsx'\n",
    "\n",
    "df_initial = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>OD Commission (including Rewards)</th>\n",
       "      <th>TP Commission (including Rewards)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Private Car</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Private Car Standalone OD</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Private Car Standalone TP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Product  OD Commission (including Rewards)  \\\n",
       "0                Private Car                               20.0   \n",
       "1  Private Car Standalone OD                               20.0   \n",
       "2  Private Car Standalone TP                                NaN   \n",
       "\n",
       "   TP Commission (including Rewards)  \n",
       "0                               15.0  \n",
       "1                                NaN  \n",
       "2                               15.0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_initial.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the first row that appears to be part of the table structure\n",
    "def find_table_start(df):\n",
    "    for idx, row in df.iterrows():\n",
    "        if row.notnull().sum() >= 2 and not all(row.astype(str).str.contains('^Unnamed', na=False)):  # At least 2 non-null values and not all 'Unnamed'\n",
    "            return idx\n",
    "    return None\n",
    "\n",
    "## to find the first numerical row \n",
    "def find_first_numerical_row(df):\n",
    "    for idx, row in df.iterrows():\n",
    "        # Check if any value in the row is numerical (int, float) or a digit-like string\n",
    "        if row.apply(lambda x: not pd.isna(x) and (isinstance(x, (int, float)) or (isinstance(x, str) and (x.isdigit() or (x.endswith('%')))))).any():\n",
    "            return idx\n",
    "    return None\n",
    "\n",
    "# to find the name of the column from which the numerical value start\n",
    "# def find_row_with_most_numerical_values(df):\n",
    "#     max_count = 0\n",
    "#     row_with_most_numericals = None\n",
    "#     first_numerical_column_name = None\n",
    "    \n",
    "#     for idx, row in df.iterrows():\n",
    "#         # Check if any value in the row is numerical (int, float) or a digit-like string\n",
    "#         is_numerical = row.apply(lambda x: not pd.isna(x) and (isinstance(x, (int, float)) or (isinstance(x, str) and x.isdigit())))\n",
    "        \n",
    "#         # Count the number of numerical values in the row\n",
    "#         numerical_count = is_numerical.sum()\n",
    "        \n",
    "#         if numerical_count > max_count:\n",
    "#             max_count = numerical_count\n",
    "#             row_with_most_numericals = idx\n",
    "            \n",
    "#             # Find the first column name with a numerical value in this row\n",
    "#             first_numerical_column = row.index[is_numerical]\n",
    "#             if len(first_numerical_column) > 0:\n",
    "#                 first_numerical_column_name = first_numerical_column[0]\n",
    "#             else:\n",
    "#                 first_numerical_column_name = None\n",
    "    \n",
    "#     return row_with_most_numericals, first_numerical_column_name\n",
    "\n",
    "def extract_numerical_value(value):\n",
    "    # Regular expression to match numeric values, optional space after '-', and optional '%' at the end\n",
    "    pattern = re.compile(r'-\\s*\\d+(\\.\\d+)?%?$|^\\d+(\\.\\d+)?%?$')\n",
    "    match = pattern.search(str(value))\n",
    "    return match.group() if match else None\n",
    "\n",
    "def find_row_with_most_numerical_values(df):\n",
    "    max_count = 0\n",
    "    row_with_most_numericals = None\n",
    "    first_numerical_column_name = None\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        # Extract numerical parts from each cell if present\n",
    "        is_numerical = row.apply(lambda x: not pd.isna(x) and (\n",
    "            isinstance(x, (int, float)) or\n",
    "            extract_numerical_value(x) is not None\n",
    "        ))\n",
    "        \n",
    "        # Count the number of numerical values in the row\n",
    "        numerical_count = is_numerical.sum()\n",
    "        \n",
    "        if numerical_count > max_count:\n",
    "            max_count = numerical_count\n",
    "            row_with_most_numericals = idx\n",
    "            \n",
    "            # Find the first column name with a numerical value in this row\n",
    "            first_numerical_column = row.index[is_numerical]\n",
    "            # print(first_numerical_column)\n",
    "            if len(first_numerical_column) > 0:\n",
    "                if first_numerical_column[0] == df.columns[0]:\n",
    "                    first_numerical_column_name = first_numerical_column[1] if len(first_numerical_column) > 1 else None\n",
    "                else:\n",
    "                    first_numerical_column_name = first_numerical_column[0]\n",
    "                # print(first_numerical_column_name)\n",
    "            else:\n",
    "                first_numerical_column_name = None\n",
    "    \n",
    "    return row_with_most_numericals, first_numerical_column_name\n",
    "\n",
    "\n",
    "# def find_all_numerical_columns(df):\n",
    "#     numerical_columns = []\n",
    "\n",
    "#     # Iterate over each column\n",
    "#     for col in df.columns:\n",
    "#         # Identify the values in the column that match the condition\n",
    "#         matching_mask = df[col].apply(\n",
    "#             lambda x: not pd.isna(x) and (\n",
    "#                 isinstance(x, (int, float)) or\n",
    "#                 (isinstance(x, str) and (x.isdigit() or (x.endswith('%'))))\n",
    "#             )\n",
    "#         )\n",
    "#         if matching_mask.any():\n",
    "#             numerical_columns.append(col)\n",
    "    \n",
    "#     # Return the first numerical column if any are found, along with the list of all numerical columns\n",
    "#     if numerical_columns:\n",
    "#         first_numerical_column = numerical_columns[0]\n",
    "#         return first_numerical_column, numerical_columns\n",
    "#     else:\n",
    "#         return None, []\n",
    "\n",
    "def clean_text(x):\n",
    "    if isinstance(x, str):\n",
    "        # Replace newline characters with a space\n",
    "        x = x.replace('\\n', ' ').replace('\\r', ' ')\n",
    "        # Remove extra spaces (multiple spaces replaced by a single space)\n",
    "        x = ' '.join(x.split())\n",
    "    return x\n",
    "\n",
    "def clean_column_headers(df):\n",
    "    new_columns = []\n",
    "    for col in df.columns:\n",
    "        if col.startswith('Unnamed'):\n",
    "            # Strip 'Unnamed' prefix and leading/trailing spaces\n",
    "            new_col = col.split('__', 1)[-1].strip()\n",
    "        else:\n",
    "            new_col = col\n",
    "        new_columns.append(new_col)\n",
    "    df.columns = new_columns\n",
    "    return df\n",
    "    \n",
    "\n",
    "# Function to clean the text inside parentheses\n",
    "def clean_parentheses_content(match):\n",
    "    content = match.group(1)\n",
    "    # cleaned_content = re.sub(r'[^\\w\\s]', '', content)  # Remove unwanted symbols\n",
    "    cleaned_content = re.sub(r'\\*', '', content)\n",
    "    cleaned_content = cleaned_content.strip()  # Remove leading and trailing spaces\n",
    "    return f\"_{cleaned_content}\"\n",
    "\n",
    "# Function to process text\n",
    "def replace_brackets(text):\n",
    "    if isinstance(text, str):\n",
    "        # Replace parentheses and remove unwanted symbols within them\n",
    "        text = re.sub(r'\\((.*?)\\)', clean_parentheses_content, text)\n",
    "        # Remove extra spaces around underscores\n",
    "        text = re.sub(r'\\s*_\\s*', '_', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Function to find the common prefix among a list of strings\n",
    "def common_prefix(strings):\n",
    "    if not strings:\n",
    "        return \"\"\n",
    "    # Start with the first string in the list as the common prefix\n",
    "    prefix = strings[0]\n",
    "    for s in strings[1:]:\n",
    "        # Compare the current prefix with each string\n",
    "        while s[:len(prefix)] != prefix and prefix:\n",
    "            # Reduce the prefix by one character at a time\n",
    "            prefix = prefix[:-1]\n",
    "    return prefix\n",
    "\n",
    "# Function to rename columns with condition\n",
    "def rename_columns(df):\n",
    "    cols = list(df.columns)\n",
    "    # Exclude columns that start with the exclude_prefix from prefix comparison\n",
    "    cols_to_check = [col for col in cols if not col.startswith('variable')]\n",
    "    \n",
    "    common_prefix_str = common_prefix(cols_to_check)\n",
    "    \n",
    "    new_cols = []\n",
    "    for col in cols:\n",
    "        if col.startswith('variable'):\n",
    "            new_cols.append(col)\n",
    "        elif col.startswith(common_prefix_str):\n",
    "            new_cols.append(col[len(common_prefix_str):])\n",
    "        else:\n",
    "            new_cols.append\n",
    "    # print(new_cols)\n",
    "    return new_cols\n",
    "\n",
    "\n",
    "def strip_spaces(text):\n",
    "    if isinstance(text, str):\n",
    "        return text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def find_last_numerical_row(df, exclude_columns=[]):\n",
    "    for idx, row in df.iloc[::-1].iterrows():\n",
    "        # Filter out the excluded columns\n",
    "        filtered_row = row.drop(labels=exclude_columns)\n",
    "        # Check if any value in the filtered row is numerical (int, float) or a digit-like string\n",
    "        if filtered_row.apply(lambda x: not pd.isna(x) and (isinstance(x, (int, float)) or (isinstance(x, str) and (x.isdigit() or x.endswith('%'))))).any():\n",
    "            return idx\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "# def find_all_prefixes(columns):\n",
    "#     exclude_prefixes = 'variable'\n",
    "#     prefixes = set()\n",
    "#     for col in columns:\n",
    "#         if any(col.startswith(exclude) for exclude in exclude_prefixes):\n",
    "#             continue\n",
    "#         parts = col.split('_')\n",
    "#         for i in range(1, len(parts)):\n",
    "#             prefix = '_'.join(parts[:i]) + '_'\n",
    "#             prefixes.add(prefix)\n",
    "#     return prefixes\n",
    "\n",
    "# def remove_prefixes(df, prefixes):\n",
    "#     # exclude_prefixes = 'variable'\n",
    "#     new_columns = []\n",
    "#     for col in df.columns:\n",
    "#         new_col = col\n",
    "#         for prefix in prefixes:\n",
    "#             new_col = new_col.replace(prefix, '')\n",
    "#         if new_col.startswith('_'):\n",
    "#             new_col = new_col.lstrip('_')\n",
    "#         new_columns.append(new_col)\n",
    "#     df.columns = new_columns\n",
    "#     # print(new_columns)\n",
    "#     return df\n",
    "\n",
    "\n",
    "def find_all_prefixes(columns, exclude_prefixes=['variable']):\n",
    "    prefixes = set()\n",
    "    for col in columns:\n",
    "        if any(col.startswith(exclude) for exclude in exclude_prefixes):\n",
    "            continue\n",
    "        parts = col.split('__')\n",
    "        for i in range(1, len(parts)):\n",
    "            prefix = '__'.join(parts[:i]) + '__'\n",
    "            prefixes.add(prefix)\n",
    "    return prefixes\n",
    "\n",
    "def remove_prefixes(df, prefixes):\n",
    "    new_columns = []\n",
    "    for col in df.columns:\n",
    "        new_col = col\n",
    "        for prefix in sorted(prefixes, key=len, reverse=True):  # Sort by length to avoid partial replacements\n",
    "            if new_col.startswith(prefix):\n",
    "                new_col = new_col[len(prefix):]\n",
    "                break  # Only remove the longest matching prefix\n",
    "        new_columns.append(new_col)\n",
    "    df.columns = new_columns\n",
    "    return df\n",
    "\n",
    "\n",
    "def find_last_numerical_row_fordrop(df):\n",
    "    for idx, row in df.iloc[::-1].iterrows():\n",
    "        # Check if any value in the row is numerical (int, float) or a digit-like string\n",
    "        if row.apply(lambda x: not pd.isna(x) and (isinstance(x, (int, float)) or (isinstance(x, str) and (x.isdigit() or x.endswith('%'))))).any():\n",
    "            return idx\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Check the first numerical row index of df_initial before converting and preprocess the data\n",
    "first_num_row_condition = find_first_numerical_row(df_initial) \n",
    "\n",
    "if first_num_row_condition > 0:\n",
    "    # Step 2: Check if all initial headers are 'Unnamed'\n",
    "    if all(df_initial.columns.astype(str).str.contains('^Unnamed', na=False)):\n",
    "        # All initial headers are 'Unnamed', find the table start row using df_initial\n",
    "        table_start_idx = find_table_start(df_initial)\n",
    "        \n",
    "        # If table_start_idx is None, there is no valid table structure in the file\n",
    "        if table_start_idx is None:\n",
    "            raise ValueError(\"No valid table structure found in the file.\")\n",
    "        \n",
    "        # Adjust df_initial to start from the detected table start row\n",
    "        df = df_initial.iloc[table_start_idx + 1:].reset_index(drop=True)\n",
    "\n",
    "        # print(df)\n",
    "        df.columns = df_initial.iloc[table_start_idx]\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        # # Get the current column names\n",
    "        columns = df.columns\n",
    "\n",
    "        # # Create a list to store the new column names with forward fill\n",
    "        new_columns = [columns[0]]  # Start with the first column name\n",
    "\n",
    "        # Iterate through the columns and forward fill wherer the column name is null or nan for this case only\n",
    "        for col in columns[1:]:\n",
    "            if pd.isna(col):\n",
    "                new_columns.append(new_columns[-1])  # Use the last valid column name\n",
    "            else:\n",
    "                new_columns.append(col)\n",
    "\n",
    "        # # Assign the new column names to the DataFrame\n",
    "        df.columns = new_columns       \n",
    "\n",
    "        #         # Step 2: Initialize a list to store notes\n",
    "        # notes = []\n",
    "        # #print(notes)\n",
    "\n",
    "        # # Step 3: Iterate through each row in the DataFrame to separate notes and remove them from df\n",
    "        # index_to_drop = []  # To store indices of rows to drop from df\n",
    "        # # for index, row in df.iterrows():\n",
    "        # #     if pd.notna(row[0]) and not any(row.dropna().index[1:]):  # Check if it's a note row\n",
    "        # #         notes.append(row[0])  # Append note to notes list\n",
    "        # #         index_to_drop.append(index)  # Mark row index to drop\n",
    "\n",
    "        # for index, row in df.iterrows():\n",
    "        #     if (pd.notna(row[0]) or pd.notna(row[1])) and not any(row.dropna().index[2:]):  # Check if it's a note row\n",
    "        #         notes.append(row[0] if pd.notna(row[0]) else row[1])  # Append note to notes list\n",
    "        #         index_to_drop.append(index) \n",
    "\n",
    "        # # Step 4: Keep rows up to and including the smallest index in index_to_drop\n",
    "        # if index_to_drop:\n",
    "        #     min_index = min(index_to_drop)  # Find the smallest index to keep\n",
    "        #     df = df.iloc[:min_index].reset_index(drop=True)  # Keep rows from the beginning up to but not including this index\n",
    "\n",
    "        # # Step 2: Drop rows with all NaN values\n",
    "        df = df.dropna(how='all').reset_index(drop=True)\n",
    "\n",
    "        # # Drop columns where all values are NaN\n",
    "        df = df.dropna(axis=1, how='all')\n",
    "\n",
    "        # Step 3: Find the first numerical row\n",
    "        first_num_row_idx = find_first_numerical_row(df)\n",
    "\n",
    "        last_numerical_idx = find_last_numerical_row(df)\n",
    "\n",
    "\n",
    "\n",
    "        if last_numerical_idx is not None:\n",
    "                df = df.iloc[:last_numerical_idx + 1]\n",
    "\n",
    "        #         # # Step 2: Drop rows with all NaN values\n",
    "        # df = df.dropna(how='all').reset_index(drop=True)\n",
    "\n",
    "        # # # Drop columns where all values are NaN\n",
    "        # df = df.dropna(axis=1, how='all')\n",
    "\n",
    "            # Process only if the first numerical row is not the first row of the DataFrame\n",
    "        if first_num_row_idx > 0:\n",
    "            # Step 4: Extract original headers and rows above the first numerical row\n",
    "            original_headers = df.columns.tolist()\n",
    "            above_rows = df.iloc[:first_num_row_idx]\n",
    "\n",
    "            ## for the Filling null value in row by last valide value to get proper header \n",
    "            above_rows_filled = above_rows.fillna(method='ffill', axis=1)\n",
    "\n",
    "            ## Assign the column header propely to use for the further work\n",
    "            df.iloc[:first_num_row_idx] = above_rows_filled\n",
    "\n",
    "            # Collect all values from above rows into a single list\n",
    "            above_rows_values = above_rows.values.flatten().tolist()\n",
    "\n",
    "            # Create combined headers by appending values above to the original headers\n",
    "            # combined_headers = [f\"{str(header)}_{str(value)}\" if not pd.isna(value) else header for header, value in zip(original_headers, above_rows_values)]\n",
    "            \n",
    "            ## below code to  handle the  if single or multiple row under the above rows to create proper Header of columns\n",
    "            combined_headers = original_headers[:]  # Start with original headers\n",
    "            for idx, row in above_rows.iterrows():\n",
    "                combined_headers = [\n",
    "                    f\"{header}__{str(value)}\" if not pd.isna(value) else header\n",
    "                    for header, value in zip(combined_headers, row)\n",
    "                ]\n",
    "\n",
    "            # Create new DataFrame with combined headers\n",
    "            new_df = pd.DataFrame(df.iloc[first_num_row_idx:].values, columns=combined_headers)\n",
    "\n",
    "            new_df = new_df.applymap(clean_text) ## for multiline text or wrap text in row or cell\n",
    "\n",
    "            # Apply the cleaning function to the column names\n",
    "            new_df.columns = [clean_text(col) for col in new_df.columns] ## removing text of multiline and wrap from heading\n",
    "\n",
    "            # Replace NaN column headers with 'type'\n",
    "            new_df.columns = ['Policy_type' if pd.isna(col) else col for col in new_df.columns]\n",
    "\n",
    "            new_df = clean_column_headers(new_df)\n",
    "\n",
    "            # Call the function\n",
    "            first_numerical_row_idx, columns_with_numerical_values = find_row_with_most_numerical_values(new_df)\n",
    "            # columns_with_numerical_values, all_numerical_columns = find_all_numerical_columns(new_df)\n",
    "            \n",
    "            col_idx = new_df.columns.get_loc(columns_with_numerical_values)\n",
    "            columns_before = new_df.columns[:col_idx]\n",
    "\n",
    "            # Step 1: Identify duplicated columns\n",
    "            duplicate_columns = []\n",
    "\n",
    "            for col in columns_before:\n",
    "                if col in new_df.columns:\n",
    "                    for other_col in new_df.columns:\n",
    "                        if col != other_col and new_df[col].equals(new_df[other_col]):\n",
    "                            duplicate_columns.append(other_col)\n",
    "\n",
    "            # Step 2: Drop duplicate columns\n",
    "            new_df.drop(columns=duplicate_columns, inplace=True)\n",
    "\n",
    "            new_df[columns_before] = new_df[columns_before].fillna(method='ffill')\n",
    "\n",
    "            last_numerical_row = find_last_numerical_row(new_df, columns_before)\n",
    "\n",
    "            \n",
    "\n",
    "            for col in new_df.columns[col_idx:]:\n",
    "                new_df[col] = new_df[col].apply(lambda x: f\"{float(x) * 100:.2f}\" if isinstance(x, (int, float)) and str(x).startswith('0.') else x)  #% symbol removing and value at time of read convert to decimal reconvert to same\n",
    "\n",
    "\n",
    "            # new_df = new_df.applymap(clean_text) ## for multiline text or wrap text in row or cell\n",
    "\n",
    "            # # Apply the cleaning function to the column names\n",
    "            # new_df.columns = [clean_text(col) for col in new_df.columns] ## removing text of multiline and wrap from heading\n",
    "\n",
    "                # Loop through the columns and rename if they start with 'All_Broker_'\n",
    "            new_columns = {col: col.split('__')[0] for col in new_df.columns if col.lower().startswith('all broker -')}\n",
    "\n",
    "            # Rename the columns in the DataFrame\n",
    "            new_df.rename(columns=new_columns, inplace=True)\n",
    "\n",
    "            # Melt the DataFrame\n",
    "            value_vars = new_df.columns[col_idx:]\n",
    "\n",
    "            melted_df = pd.melt(new_df, id_vars=list(columns_before), value_vars=value_vars, var_name='variable', value_name='commission')\n",
    "\n",
    "            # Split the 'variable' column based on '/' and '_' and expand into separate columns\n",
    "            # expanded_cols = melted_df['variable'].str.split(r'[_]', expand=True)\n",
    "            expanded_cols = melted_df['variable'].str.split(r'__', expand=True)\n",
    "            \n",
    "            # Rename columns based on the maximum number of splits\n",
    "            expanded_cols.columns = [f'variable_{i+1}' for i in range(expanded_cols.shape[1])]\n",
    "            \n",
    "            # Concatenate the expanded columns to the melted DataFrame\n",
    "            melted_df = pd.concat([melted_df, expanded_cols], axis=1)\n",
    "            \n",
    "            # Rearrange columns: expanded columns after id_vars and before 'commission'\n",
    "            id_vars = list(columns_before)\n",
    "            value_name = 'commission'\n",
    "            \n",
    "            columns_order = id_vars + [col for col in expanded_cols.columns] + [value_name]\n",
    "            melted_df = melted_df[columns_order]\n",
    "\n",
    "\n",
    "                        # Remove rows where the commission value is a substring of any value in the id_vars columns\n",
    "            rows_to_drop = melted_df.apply(\n",
    "                lambda row: any(str(row[value_name]) in str(row[id_var]) for id_var in id_vars), axis=1)\n",
    "\n",
    "            melted_df = melted_df[~rows_to_drop].reset_index(drop=True)\n",
    "\n",
    "            \n",
    "            melted_df.columns = [col.split('__')[-1] if col.lower().startswith('all broker') else col for col in melted_df.columns]\n",
    "            melted_df= melted_df.dropna(subset=['commission']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "            melted_df.columns = rename_columns(melted_df)\n",
    "            # Apply the function to the entire DataFrame\n",
    "            melted_df = melted_df.applymap(replace_brackets)\n",
    "\n",
    "            # # Remove rows where any value in the 'commission' column matches any value in the 'id_vars' columns\n",
    "            # rows_to_drop = melted_df.apply(\n",
    "            #     lambda row: row[id_vars].isin([row[value_name]]).any(), axis=1)\n",
    "\n",
    "            # melted_df = melted_df[~rows_to_drop].reset_index(drop=True)\n",
    "\n",
    "            # Apply the function to the entire DataFrame\n",
    "            melted_df = melted_df.applymap(strip_spaces)\n",
    "\n",
    "\n",
    "            # Find all possible prefixes excluding certain prefixes\n",
    "            all_prefixes = find_all_prefixes(melted_df.columns)\n",
    "            # Remove all identified prefixes\n",
    "            melted_df = remove_prefixes(melted_df, all_prefixes)\n",
    "\n",
    "        \n",
    "        else:\n",
    "            # Step 2: Drop rows with all NaN values\n",
    "            df = df.dropna(how='all').reset_index(drop=True)\n",
    "\n",
    "            # Drop columns where all values are NaN\n",
    "            df = df.dropna(axis=1, how='all')\n",
    "\n",
    "            # # Step 2: Initialize a list to store notes\n",
    "            # notes = []\n",
    "            # #print(notes)\n",
    "\n",
    "            # # Step 3: Iterate through each row in the DataFrame to separate notes and remove them from df\n",
    "            # index_to_drop = []  # To store indices of rows to drop from df\n",
    "            # # for index, row in df.iterrows():\n",
    "            # #     if pd.notna(row[0]) and not any(row.dropna().index[1:]):  # Check if it's a note row\n",
    "            # #         notes.append(row[0])  # Append note to notes list\n",
    "            # #         index_to_drop.append(index)  # Mark row index to drop\n",
    "\n",
    "            # for index, row in df.iterrows():\n",
    "            #     if (pd.notna(row[0]) or pd.notna(row[1])) and not any(row.dropna().index[2:]):  # Check if it's a note row\n",
    "            #         notes.append(row[0] if pd.notna(row[0]) else row[1])  # Append note to notes list\n",
    "            #         index_to_drop.append(index) \n",
    "\n",
    "            # # Step 4: Keep rows up to and including the smallest index in index_to_drop\n",
    "            # if index_to_drop:\n",
    "            #     min_index = min(index_to_drop)  # Find the smallest index to keep\n",
    "            #     df = df.iloc[:min_index].reset_index(drop=True)  # Keep rows from the beginning up to but not including this index\n",
    "\n",
    "\n",
    "            new_df = df.copy()\n",
    "\n",
    "            new_df = new_df.applymap(clean_text) ## for multiline text or wrap text in row or cell\n",
    "\n",
    "            # Apply the cleaning function to the column names\n",
    "            new_df.columns = [clean_text(col) for col in new_df.columns] ## removing text of multiline and wrap from heading\n",
    "\n",
    "\n",
    "            new_df = clean_column_headers(new_df)\n",
    "\n",
    "            # Call the function\n",
    "            first_numerical_row_idx, columns_with_numerical_values = find_row_with_most_numerical_values(new_df)\n",
    "            # columns_with_numerical_values, all_numerical_columns = find_all_numerical_columns(new_df)\n",
    "            \n",
    "            col_idx = new_df.columns.get_loc(columns_with_numerical_values)\n",
    "            columns_before = new_df.columns[:col_idx]\n",
    "\n",
    "            new_df[columns_before] = new_df[columns_before].fillna(method='ffill')\n",
    "\n",
    "            last_numerical_row = find_last_numerical_row(new_df, columns_before)\n",
    "\n",
    "            for col in new_df.columns[col_idx:]:\n",
    "                new_df[col] = new_df[col].apply(lambda x: f\"{float(x) * 100:.2f}\" if isinstance(x, (int, float)) and str(x).startswith('0.') else x)  #% symbol removing and value at time of read convert to decimal reconvert to same\n",
    "\n",
    "\n",
    "            # new_df = new_df.applymap(clean_text) ## for multiline text or wrap text in row or cell\n",
    "\n",
    "            # # Apply the cleaning function to the column names\n",
    "            # new_df.columns = [clean_text(col) for col in new_df.columns] ## removing text of multiline and wrap from heading\n",
    "\n",
    "\n",
    "                # Loop through the columns and rename if they start with 'All_Broker_'\n",
    "            new_columns = {col: col.split('__')[0] for col in df.columns if col.lower().startswith('all broker -')}\n",
    "\n",
    "            # Rename the columns in the DataFrame\n",
    "            new_df.rename(columns=new_columns, inplace=True)\n",
    "\n",
    "            # Melt the DataFrame\n",
    "            value_vars = new_df.columns[col_idx:]\n",
    "            melted_df = pd.melt(new_df, id_vars=list(columns_before), value_vars=value_vars, var_name='variable', value_name='commission')\n",
    "\n",
    "            # Split the 'variable' column based on '/' and '_' and expand into separate columns\n",
    "            # expanded_cols = melted_df['variable'].str.split(r'[_]', expand=True)\n",
    "            expanded_cols = melted_df['variable'].str.split(r'__', expand=True)\n",
    "            \n",
    "            # Rename columns based on the maximum number of splits\n",
    "            expanded_cols.columns = [f'variable_{i+1}' for i in range(expanded_cols.shape[1])]\n",
    "            \n",
    "            # Concatenate the expanded columns to the melted DataFrame\n",
    "            melted_df = pd.concat([melted_df, expanded_cols], axis=1)\n",
    "            \n",
    "            # Rearrange columns: expanded columns after id_vars and before 'commission'\n",
    "            id_vars = list(columns_before)\n",
    "            value_name = 'commission'\n",
    "            \n",
    "            columns_order = id_vars + [col for col in expanded_cols.columns] + [value_name]\n",
    "            melted_df = melted_df[columns_order]\n",
    "\n",
    "\n",
    "                        # Remove rows where the commission value is a substring of any value in the id_vars columns\n",
    "            rows_to_drop = melted_df.apply(\n",
    "                lambda row: any(str(row[value_name]) in str(row[id_var]) for id_var in id_vars), axis=1)\n",
    "\n",
    "            melted_df = melted_df[~rows_to_drop].reset_index(drop=True)\n",
    "\n",
    "            melted_df.columns = [col.split('__')[-1] if col.lower().startswith('all broker') else col for col in melted_df.columns]\n",
    "            melted_df= melted_df.dropna(subset=['commission']).reset_index(drop=True) \n",
    "\n",
    "            melted_df.columns = rename_columns(melted_df) \n",
    "            # Apply the function to the entire DataFrame\n",
    "            melted_df = melted_df.applymap(replace_brackets)\n",
    "\n",
    "            # Apply the function to the entire DataFrame\n",
    "            melted_df = melted_df.applymap(strip_spaces)\n",
    "\n",
    "            # Find all possible prefixes excluding certain prefixes\n",
    "            all_prefixes = find_all_prefixes(melted_df.columns)\n",
    "            # Remove all identified prefixes\n",
    "            melted_df = remove_prefixes(melted_df, all_prefixes)\n",
    "\n",
    "    else:\n",
    "        df =  df_initial.copy()\n",
    "\n",
    "        # Step 1: Replace null or unnamed headers with inferred names (if needed)\n",
    "        unnamed_columns = df.columns.str.contains('^Unnamed')\n",
    "        new_columns = []\n",
    "        last_valid_header = None\n",
    "\n",
    "        for i, col in enumerate(df.columns):\n",
    "            if unnamed_columns[i]:\n",
    "                if last_valid_header is not None:\n",
    "                    new_columns.append(last_valid_header)\n",
    "                else:\n",
    "                    new_columns.append('Unnamed')\n",
    "            else:\n",
    "                new_columns.append(col)\n",
    "                last_valid_header = col\n",
    "\n",
    "        df.columns = new_columns\n",
    "\n",
    "        # Step 2: Drop rows with all NaN values as well as column also with complete NaN value\n",
    "        df = df.dropna(how='all').reset_index(drop=True) # for row drop\n",
    "        df = df.dropna(axis=1, how='all') # for Column drop\n",
    "\n",
    "        # # Step 2: Initialize a list to store notes\n",
    "        # notes = []\n",
    "        # #print(notes)\n",
    "\n",
    "        # # Step 3: Iterate through each row in the DataFrame to separate notes and remove them from df\n",
    "        # index_to_drop = []  # To store indices of rows to drop from df\n",
    "        # # for index, row in df.iterrows():\n",
    "        # #     if pd.notna(row[0]) and not any(row.dropna().index[1:]):  # Check if it's a note row\n",
    "        # #         notes.append(row[0])  # Append note to notes list\n",
    "        # #         index_to_drop.append(index)  # Mark row index to drop\n",
    "\n",
    "        # for index, row in df.iterrows():\n",
    "        #     if (pd.notna(row[0]) or pd.notna(row[1])) and not any(row.dropna().index[2:]):  # Check if it's a note row\n",
    "        #         notes.append(row[0] if pd.notna(row[0]) else row[1])  # Append note to notes list\n",
    "        #         index_to_drop.append(index) \n",
    "\n",
    "        # # Step 4: Keep rows up to and including the smallest index in index_to_drop\n",
    "        # if index_to_drop:\n",
    "        #     min_index = min(index_to_drop)  # Find the smallest index to keep\n",
    "        #     df = df.iloc[:min_index].reset_index(drop=True)  # Keep rows from the beginning up to but not including this index\n",
    "\n",
    "        # Step 3: Find the first numerical row\n",
    "        first_num_row_idx = find_first_numerical_row(df)\n",
    "\n",
    "        # Process only if the first numerical row is not the first row of the DataFrame\n",
    "        if first_num_row_idx > 0:\n",
    "            # Step 4: Extract original headers and rows above the first numerical row\n",
    "            original_headers = df.columns.tolist()\n",
    "            above_rows = df.iloc[:first_num_row_idx]\n",
    "\n",
    "            # to fill the row horizontaly to make proper header    \n",
    "            above_rows_filled = above_rows.fillna(method='ffill', axis=1)\n",
    "\n",
    "            # assigning the value find in above step to make proper header\n",
    "            df.iloc[:first_num_row_idx] = above_rows_filled\n",
    "\n",
    "            # Collect all values from above rows into a single list\n",
    "            above_rows_values = above_rows.values.flatten().tolist()\n",
    "\n",
    "            # Create combined headers by appending values above to the original headers\n",
    "            # combined_headers = [f\"{str(header)}_{str(value)}\" if not pd.isna(value) else header for header, value in zip(original_headers, above_rows_values)]\n",
    "\n",
    "            ## the Below code is useful to handle if there is more than one row in the above row to proper fill\n",
    "            combined_headers = original_headers[:]  # Start with original headers\n",
    "            for idx, row in above_rows.iterrows():\n",
    "                combined_headers = [\n",
    "                    f\"{header}__{str(value)}\" if not pd.isna(value) else header\n",
    "                    for header, value in zip(combined_headers, row)\n",
    "                ]\n",
    "\n",
    "            # Create new DataFrame with combined headers\n",
    "            new_df = pd.DataFrame(df.iloc[first_num_row_idx:].values, columns=combined_headers)\n",
    "\n",
    "            new_df = new_df.applymap(clean_text) ## for multiline text or wrap text in row or cell\n",
    "\n",
    "            # Apply the cleaning function to the column names\n",
    "            new_df.columns = [clean_text(col) for col in new_df.columns] ## removing text of multiline and wrap from heading\n",
    "\n",
    "            new_df = clean_column_headers(new_df)\n",
    "\n",
    "            # Call the function to find the first numerical column to use melted df\n",
    "            first_numerical_row_idx, columns_with_numerical_values = find_row_with_most_numerical_values(new_df)\n",
    "            # columns_with_numerical_values, all_numerical_columns = find_all_numerical_columns(new_df)\n",
    "                \n",
    "            col_idx = new_df.columns.get_loc(columns_with_numerical_values)\n",
    "            columns_before = new_df.columns[:col_idx]\n",
    "\n",
    "            new_df[columns_before] = new_df[columns_before].fillna(method='ffill')\n",
    "\n",
    "            last_numerical_row = find_last_numerical_row(new_df, columns_before)\n",
    "\n",
    "            for col in new_df.columns[col_idx:]:\n",
    "                new_df[col] = new_df[col].apply(lambda x: f\"{float(x) * 100:.2f}\" if isinstance(x, (int, float)) and str(x).startswith('0.') else x)  #% symbol removing and value at time of read convert to decimal reconvert to same\n",
    "\n",
    "\n",
    "            # new_df = new_df.applymap(clean_text) ## for multiline text or wrap text in row or cell\n",
    "\n",
    "            #     # Apply the cleaning function to the column names\n",
    "            # new_df.columns = [clean_text(col) for col in new_df.columns] ## removing text of multiline and wrap from heading\n",
    "\n",
    "                # Loop through the columns and rename if they start with 'All_Broker_'\n",
    "            new_columns = {col: col.split('__')[0] for col in df.columns if col.lower().startswith('all broker -')}\n",
    "\n",
    "            # Rename the columns in the DataFrame\n",
    "            new_df.rename(columns=new_columns, inplace=True)\n",
    "\n",
    "                # Melt the DataFrame\n",
    "            value_vars = new_df.columns[col_idx:]\n",
    "            melted_df = pd.melt(new_df, id_vars=list(columns_before), value_vars=value_vars, var_name='variable', value_name='commission')\n",
    "\n",
    "                # Split the 'variable' column based on '/' and '_' and expand into separate columns\n",
    "            # expanded_cols = melted_df['variable'].str.split(r'[_]', expand=True)\n",
    "            expanded_cols = melted_df['variable'].str.split(r'__', expand=True)\n",
    "                \n",
    "                # Rename columns based on the maximum number of splits\n",
    "            expanded_cols.columns = [f'variable_{i+1}' for i in range(expanded_cols.shape[1])]\n",
    "                \n",
    "                # Concatenate the expanded columns to the melted DataFrame\n",
    "            melted_df = pd.concat([melted_df, expanded_cols], axis=1)\n",
    "                \n",
    "                # Rearrange columns: expanded columns after id_vars and before 'commission'\n",
    "            id_vars = list(columns_before)\n",
    "            value_name = 'commission'\n",
    "                \n",
    "            columns_order = id_vars + [col for col in expanded_cols.columns] + [value_name]\n",
    "            melted_df = melted_df[columns_order]\n",
    "\n",
    "                        # Remove rows where the commission value is a substring of any value in the id_vars columns\n",
    "            rows_to_drop = melted_df.apply(\n",
    "                lambda row: any(str(row[value_name]) in str(row[id_var]) for id_var in id_vars), axis=1)\n",
    "\n",
    "            melted_df = melted_df[~rows_to_drop].reset_index(drop=True)\n",
    "\n",
    "            melted_df.columns = [col.split('__')[-1] if col.lower().startswith('all broker') else col for col in melted_df.columns]\n",
    "            melted_df= melted_df.dropna(subset=['commission']).reset_index(drop=True)   \n",
    "            \n",
    "            melted_df.columns = rename_columns(melted_df)\n",
    "            # Apply the function to the entire DataFrame\n",
    "            melted_df = melted_df.applymap(replace_brackets)\n",
    "\n",
    "            # Apply the function to the entire DataFrame\n",
    "            melted_df = melted_df.applymap(strip_spaces)\n",
    "\n",
    "            # Find all possible prefixes excluding certain prefixes\n",
    "            all_prefixes = find_all_prefixes(melted_df.columns)\n",
    "            # Remove all identified prefixes\n",
    "            melted_df = remove_prefixes(melted_df, all_prefixes)\n",
    "\n",
    "        else:\n",
    "            # Step 2: Drop rows with all NaN values\n",
    "            df = df.dropna(how='all').reset_index(drop=True)\n",
    "\n",
    "            # Drop columns where all values are NaN\n",
    "            df = df.dropna(axis=1, how='all')\n",
    "\n",
    "            # # Step 2: Initialize a list to store notes\n",
    "            # notes = []\n",
    "            # #print(notes)\n",
    "\n",
    "            # # Step 3: Iterate through each row in the DataFrame to separate notes and remove them from df\n",
    "            # index_to_drop = []  # To store indices of rows to drop from df\n",
    "            # # for index, row in df.iterrows():\n",
    "            # #     if pd.notna(row[0]) and not any(row.dropna().index[1:]):  # Check if it's a note row\n",
    "            # #         notes.append(row[0])  # Append note to notes list\n",
    "            # #         index_to_drop.append(index)  # Mark row index to drop\n",
    "\n",
    "\n",
    "            # for index, row in df.iterrows():\n",
    "            #     if (pd.notna(row[0]) or pd.notna(row[1])) and not any(row.dropna().index[2:]):  # Check if it's a note row\n",
    "            #         notes.append(row[0] if pd.notna(row[0]) else row[1])  # Append note to notes list\n",
    "            #         index_to_drop.append(index) \n",
    "\n",
    "            # # Step 4: Keep rows up to and including the smallest index in index_to_drop\n",
    "            # if index_to_drop:\n",
    "            #     min_index = min(index_to_drop)  # Find the smallest index to keep\n",
    "            #     df = df.iloc[:min_index].reset_index(drop=True)  # Keep rows from the beginning up to but not including this index\n",
    "\n",
    "            new_df = df.copy()\n",
    "\n",
    "            new_df = new_df.applymap(clean_text) ## for multiline text or wrap text in row or cell\n",
    "\n",
    "            # Apply the cleaning function to the column names\n",
    "            new_df.columns = [clean_text(col) for col in new_df.columns] ## removing text of multiline and wrap from heading\n",
    "\n",
    "            new_df = clean_column_headers(new_df)\n",
    "\n",
    "            # Call the function\n",
    "            first_numerical_row_idx, columns_with_numerical_values = find_row_with_most_numerical_values(new_df)\n",
    "            # columns_with_numerical_values, all_numerical_columns = find_all_numerical_columns(new_df)\n",
    "            \n",
    "            col_idx = new_df.columns.get_loc(columns_with_numerical_values)\n",
    "            columns_before = new_df.columns[:col_idx]\n",
    "\n",
    "            new_df[columns_before] = new_df[columns_before].fillna(method='ffill')\n",
    "\n",
    "            last_numerical_row = find_last_numerical_row(new_df, columns_before)\n",
    "\n",
    "            for col in new_df.columns[col_idx:]:\n",
    "                new_df[col] = new_df[col].apply(lambda x: f\"{float(x) * 100:.2f}\" if isinstance(x, (int, float)) and str(x).startswith('0.') else x)  #% symbol removing and value at time of read convert to decimal reconvert to same\n",
    "\n",
    "\n",
    "            # new_df = new_df.applymap(clean_text) ## for multiline text or wrap text in row or cell\n",
    "\n",
    "            # # Apply the cleaning function to the column names\n",
    "            # new_df.columns = [clean_text(col) for col in new_df.columns] ## removing text of multiline and wrap from heading\n",
    "\n",
    "            # Loop through the columns and rename if they start with 'All_Broker_'\n",
    "            new_columns = {col: col.split('__')[0] for col in df.columns if col.lower().startswith('all broker -')}\n",
    "\n",
    "\n",
    "            # Rename the columns in the DataFrame\n",
    "            new_df.rename(columns=new_columns, inplace=True)\n",
    "\n",
    "            # Melt the DataFrame\n",
    "            value_vars = new_df.columns[col_idx:]\n",
    "            melted_df = pd.melt(new_df, id_vars=list(columns_before), value_vars=value_vars, var_name='variable', value_name='commission')\n",
    "\n",
    "            # Split the 'variable' column based on '/' and '_' and expand into separate columns\n",
    "            # expanded_cols = melted_df['variable'].str.split(r'[_]', expand=True)\n",
    "            expanded_cols = melted_df['variable'].str.split(r'__', expand=True)\n",
    "            \n",
    "            # Rename columns based on the maximum number of splits\n",
    "            expanded_cols.columns = [f'variable_{i+1}' for i in range(expanded_cols.shape[1])]\n",
    "            \n",
    "            # Concatenate the expanded columns to the melted DataFrame\n",
    "            melted_df = pd.concat([melted_df, expanded_cols], axis=1)\n",
    "            \n",
    "            # Rearrange columns: expanded columns after id_vars and before 'commission'\n",
    "            id_vars = list(columns_before)\n",
    "            value_name = 'commission'\n",
    "            \n",
    "            columns_order = id_vars + [col for col in expanded_cols.columns] + [value_name]\n",
    "            melted_df = melted_df[columns_order]\n",
    "\n",
    "                        # Remove rows where the commission value is a substring of any value in the id_vars columns\n",
    "            rows_to_drop = melted_df.apply(\n",
    "                lambda row: any(str(row[value_name]) in str(row[id_var]) for id_var in id_vars), axis=1)\n",
    "\n",
    "            melted_df = melted_df[~rows_to_drop].reset_index(drop=True)\n",
    "\n",
    "            melted_df.columns = [col.split('__')[-1] if col.lower().startswith('all broker') else col for col in melted_df.columns]\n",
    "\n",
    "            melted_df= melted_df.dropna(subset=['commission']).reset_index(drop=True)\n",
    "\n",
    "            melted_df.columns = rename_columns(melted_df)\n",
    "\n",
    "            # Apply the function to the entire DataFrame\n",
    "            melted_df = melted_df.applymap(replace_brackets)\n",
    "\n",
    "            # Apply the function to the entire DataFrame\n",
    "            melted_df = melted_df.applymap(strip_spaces)\n",
    "\n",
    "            # Find all possible prefixes excluding certain prefixes\n",
    "            all_prefixes = find_all_prefixes(melted_df.columns)\n",
    "            # Remove all identified prefixes\n",
    "            melted_df = remove_prefixes(melted_df, all_prefixes)\n",
    "                        \n",
    "\n",
    "else:\n",
    "    # Initial headers are valid, use the initially read DataFrame with headers as the first row\n",
    "    df = df_initial.copy()\n",
    "    #Step 2: Drop rows with all NaN values\n",
    "    df = df.dropna(how='all').reset_index(drop=True)\n",
    "\n",
    "    # Drop columns where all values are NaN\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "\n",
    "    # # Step 2: Initialize a list to store notes\n",
    "    # notes = []\n",
    "    # #print(notes)\n",
    "\n",
    "    # # Step 3: Iterate through each row in the DataFrame to separate notes and remove them from df\n",
    "    # index_to_drop = []  # To store indices of rows to drop from df\n",
    "    # # for index, row in df.iterrows():\n",
    "    # #     if pd.notna(row[0]) and not any(row.dropna().index[1:]):  # Check if it's a note row\n",
    "    # #         notes.append(row[0])  # Append note to notes list\n",
    "    # #         index_to_drop.append(index)  # Mark row index to drop\n",
    "\n",
    "    # for index, row in df.iterrows():\n",
    "    #     if (pd.notna(row[0]) or pd.notna(row[1])) and not any(row.dropna().index[2:]):  # Check if it's a note row\n",
    "    #         notes.append(row[0] if pd.notna(row[0]) else row[1])  # Append note to notes list\n",
    "    #         index_to_drop.append(index) \n",
    "\n",
    "    #     # Step 4: Keep rows up to and including the smallest index in index_to_drop\n",
    "    # if index_to_drop:\n",
    "    #     min_index = min(index_to_drop)  # Find the smallest index to keep\n",
    "    #     df = df.iloc[:min_index].reset_index(drop=True)  # Keep rows from the beginning up to but not including this index\n",
    "    \n",
    "    new_df = df.copy()\n",
    "\n",
    "\n",
    "    new_df = new_df.applymap(clean_text) ## for multiline text or wrap text in row or cell\n",
    "\n",
    "            # Apply the cleaning function to the column names\n",
    "    new_df.columns = [clean_text(col) for col in new_df.columns] ## removing text of multiline and wrap from heading\n",
    "\n",
    "    new_df = clean_column_headers(new_df)\n",
    "\n",
    "    # Call the function\n",
    "    first_numerical_row_idx, columns_with_numerical_values = find_row_with_most_numerical_values(new_df)\n",
    "    # columns_with_numerical_values, all_numerical_columns = find_all_numerical_columns(new_df)\n",
    "            \n",
    "    col_idx = new_df.columns.get_loc(columns_with_numerical_values)\n",
    "    columns_before = new_df.columns[:col_idx]\n",
    "\n",
    "    new_df[columns_before] = new_df[columns_before].fillna(method='ffill')\n",
    "\n",
    "    last_numerical_row = find_last_numerical_row(new_df, columns_before)\n",
    "\n",
    "\n",
    "\n",
    "    for col in new_df.columns[col_idx:]:\n",
    "        new_df[col] = new_df[col].apply(lambda x: f\"{float(x) * 100:.2f}\" if isinstance(x, (int, float)) and str(x).startswith('0.') else x)  #% symbol removing and value at time of read convert to decimal reconvert to same\n",
    "\n",
    "\n",
    "    # new_df = new_df.applymap(clean_text) ## for multiline text or wrap text in row or cell\n",
    "\n",
    "    # #         # Apply the cleaning function to the column names\n",
    "    # new_df.columns = [clean_text(col) for col in new_df.columns] ## removing text of multiline and wrap from heading\n",
    "\n",
    "    # Loop through the columns and rename if they start with 'All_Broker_'\n",
    "    new_columns = {col: col.split('__')[0] for col in df.columns if col.lower().startswith('all broker -')}\n",
    "\n",
    "    # Rename the columns in the DataFrame\n",
    "    new_df.rename(columns=new_columns, inplace=True)\n",
    "\n",
    "    # melted_df= new_df.dropna(subset=[columns_with_numerical_values]).reset_index(drop=True)\n",
    "    melted_df = new_df.copy()\n",
    "\n",
    "    # Define a more flexible pattern for extracting details\n",
    "    pattern = re.compile(r'Diesel upto (\\d+)cc \\(all model will cover\\)\\s*(?:and|&)?\\s*all cars above (\\d+)cc\\s*(?:except|excluding)\\s*(\\w+)',re.IGNORECASE)\n",
    "\n",
    "    # Initialize variables for extracted details\n",
    "    diesel_cc, large_cars_cc, except_model = None, None, None\n",
    "\n",
    "    # Search the entire DataFrame for the pattern in cell values\n",
    "    for cell in melted_df.values.flatten():\n",
    "        cell_str = str(cell)  # Convert cell to string for pattern matching\n",
    "        match = pattern.search(cell_str)\n",
    "        if match:\n",
    "            diesel_cc, large_cars_cc, except_model = match.groups()\n",
    "            # print(f\"Found details - diesel_cc: {diesel_cc}, large_cars_cc: {large_cars_cc}, except_model: {except_model}\")\n",
    "            break\n",
    "\n",
    "    # Define the replacement text if required values were found\n",
    "    if diesel_cc and large_cars_cc and except_model:\n",
    "        replacement_text = (f'All other Cars - Petrol upto {large_cars_cc}cc and Diesel above {diesel_cc}cc. Above {large_cars_cc}cc For {except_model}')\n",
    "        # Replace the text across the entire DataFrame\n",
    "        melted_df.replace('All other Cars except above model', replacement_text, inplace=True)\n",
    "\n",
    "    # Define the substring to replace and the replacement value\n",
    "    # substring_to_replace = \n",
    "    if except_model == 'TTMMHHK':\n",
    "        replacement_value = 'TATA|TOYOTA|MARUTI|MAHINDRA|HONDA|HYUNDAI|KIA'\n",
    "\n",
    "        # Replace the substring in the entire DataFrame\n",
    "        melted_df = melted_df.applymap(lambda x: x.replace(except_model, replacement_value) if isinstance(x, str) else x)\n",
    "    else:\n",
    "        melted_df\n",
    "\n",
    "    if columns_with_numerical_values == 'Renewal' or columns_with_numerical_values == 'commission':\n",
    "        melted_df= melted_df.dropna(subset=[columns_with_numerical_values]).reset_index(drop=True)\n",
    "        melted_df = melted_df.drop(columns= ['Comm on Net Premium of 1st year', 'Unnamed: 5'])\n",
    "\n",
    "    else:\n",
    "        melted_df\n",
    "\n",
    "\n",
    "    melted_df.columns = rename_columns(melted_df)\n",
    "\n",
    "    # Apply the function to the entire DataFrame\n",
    "    melted_df = melted_df.applymap(replace_brackets)\n",
    "\n",
    "    # # Remove rows where the commission value is a substring of any value in the id_vars columns\n",
    "    # rows_to_drop = melted_df.apply(\n",
    "    #     lambda row: any(str(row[value_name]) in str(row[id_var]) for id_var in id_vars), axis=1)\n",
    "\n",
    "    # melted_df = melted_df[~rows_to_drop].reset_index(drop=True)\n",
    "\n",
    "    # Apply the function to the entire DataFrame\n",
    "    melted_df = melted_df.applymap(strip_spaces)\n",
    "\n",
    "    # Find all possible prefixes excluding certain prefixes\n",
    "    all_prefixes = find_all_prefixes(melted_df.columns)\n",
    "    # Remove all identified prefixes\n",
    "    melted_df = remove_prefixes(melted_df, all_prefixes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_start_idx\n",
    "# columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_num_row_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>OD Commission (including Rewards)</th>\n",
       "      <th>TP Commission (including Rewards)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Private Car</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Private Car Standalone OD</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Private Car Standalone TP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Product  OD Commission (including Rewards)  \\\n",
       "0                Private Car                               20.0   \n",
       "1  Private Car Standalone OD                               20.0   \n",
       "2  Private Car Standalone TP                                NaN   \n",
       "\n",
       "   TP Commission (including Rewards)  \n",
       "0                               15.0  \n",
       "1                                NaN  \n",
       "2                               15.0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_initial.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>OD Commission (including Rewards)</th>\n",
       "      <th>TP Commission (including Rewards)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Private Car</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Private Car Standalone OD</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Product  OD Commission (including Rewards)  \\\n",
       "0                Private Car                               20.0   \n",
       "1  Private Car Standalone OD                               20.0   \n",
       "\n",
       "   TP Commission (including Rewards)  \n",
       "0                               15.0  \n",
       "1                                NaN  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>OD Commission (including Rewards)</th>\n",
       "      <th>TP Commission (including Rewards)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Private Car</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Private Car Standalone OD</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Product  OD Commission (including Rewards)  \\\n",
       "0                Private Car                               20.0   \n",
       "1  Private Car Standalone OD                               20.0   \n",
       "\n",
       "   TP Commission (including Rewards)  \n",
       "0                               15.0  \n",
       "1                                NaN  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>OD Commission (including Rewards)</th>\n",
       "      <th>TP Commission (including Rewards)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Private Car</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Private Car Standalone OD</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Product  OD Commission (including Rewards)  \\\n",
       "0                Private Car                               20.0   \n",
       "1  Private Car Standalone OD                               20.0   \n",
       "\n",
       "   TP Commission (including Rewards)  \n",
       "0                               15.0  \n",
       "1                                NaN  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melted_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "akhgklh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First numerical column: OD Commission (including Rewards)\n",
      "Index of the column: 1\n"
     ]
    }
   ],
   "source": [
    "# Function to find the first numerical column and its index\n",
    "def find_first_numerical_column(df):\n",
    "    for index, column in enumerate(df.columns):\n",
    "        # Try converting the column to numeric\n",
    "        numeric_column = pd.to_numeric(df[column], errors='coerce')\n",
    "        # print(numeric_column)\n",
    "        # Check if there are any non-NaN values\n",
    "        if numeric_column.dropna().size > 0:\n",
    "            if index == 0:\n",
    "                continue\n",
    "            return column, index\n",
    "    return None, None\n",
    "\n",
    "# def find_first_numerical_column(df):\n",
    "#     for index, column in enumerate(df.columns[1:], start=1):  # Start from the second column\n",
    "#         # Try converting the column to numeric\n",
    "#         numeric_column = pd.to_numeric(df[column], errors='coerce')\n",
    "#         # Check if there are any non-NaN values\n",
    "#         if numeric_column.dropna().size > 0:\n",
    "#             return column, index\n",
    "#     return None, None\n",
    "\n",
    "\n",
    "# Get the first numerical column name and index\n",
    "first_numerical_column, column_index = find_first_numerical_column(melted_df)\n",
    "print(f\"First numerical column: {first_numerical_column}\")\n",
    "print(f\"Index of the column: {column_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def df_to_hierarchical_json(df, num_levels):\n",
    "    \"\"\"\n",
    "    Convert a DataFrame to a hierarchical JSON format.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to convert.\n",
    "        num_levels (int): Number of hierarchical levels to extract from the start of the columns.\n",
    "\n",
    "    Returns:\n",
    "        str: JSON string of the hierarchical structure.\n",
    "    \"\"\"\n",
    "    if num_levels > 1:\n",
    "        # Ensure num_levels is not more than the number of columns\n",
    "        num_levels = min(num_levels, len(df.columns))\n",
    "        \n",
    "        # Identify hierarchical columns\n",
    "        level_columns = df.columns[:num_levels]\n",
    "        print(level_columns)\n",
    "        value_columns = df.columns[num_levels:]\n",
    "        print(value_columns)\n",
    "        \n",
    "        # Initialize the hierarchical dictionary\n",
    "        hierarchy = {}\n",
    "        \n",
    "        # Group by hierarchical levels\n",
    "        grouped = df.groupby(list(level_columns))\n",
    "        \n",
    "        # Populate the hierarchical dictionary\n",
    "        for keys, group in grouped:\n",
    "            current_level = hierarchy\n",
    "            for key in keys:\n",
    "                key = str(key)\n",
    "                if key not in current_level:\n",
    "                    current_level[key] = {}\n",
    "                current_level = current_level[key]\n",
    "            \n",
    "            # Append the values at the deepest level\n",
    "            for _, row in group.iterrows():\n",
    "                values = {col: (row[col] if not pd.isna(row[col]) else None) for col in value_columns}\n",
    "                # current_level.setdefault('values', []).append(values)\n",
    "                if 'values' not in current_level:\n",
    "                    current_level['values'] = []\n",
    "                current_level['values'].append(values)\n",
    "        \n",
    "        # Convert dictionary to JSON\n",
    "        return json.dumps(hierarchy, indent=2)\n",
    "    else:\n",
    "            # Use columns before the numerical column as hierarchical levels\n",
    "        # and columns after as value columns\n",
    "        level_columns = df.columns[:column_index]\n",
    "        value_columns = df.columns[column_index:]\n",
    "        \n",
    "        # Initialize the hierarchical dictionary\n",
    "        hierarchy = {}\n",
    "\n",
    "        # Populate the hierarchical dictionary\n",
    "        for index, row in df.iterrows():\n",
    "            current_level = hierarchy\n",
    "            # Use the entire row for hierarchical levels\n",
    "            for col in level_columns:\n",
    "                key = row[col]\n",
    "                key = str(key)\n",
    "                if key not in current_level:\n",
    "                    current_level[key] = {}\n",
    "                current_level = current_level[key]\n",
    "\n",
    "            # Append the values at the deepest level\n",
    "            values = {col: row[col] if not pd.isna(row[col]) else None for col in value_columns}\n",
    "            if 'values' not in current_level:\n",
    "                current_level['values'] = []\n",
    "            current_level['values'].append(values)\n",
    "\n",
    "        # Convert dictionary to JSON\n",
    "        return json.dumps(hierarchy, indent=2)\n",
    "\n",
    "# Prompt the user for the number of levels\n",
    "# num_levels = min(5, len(df.columns))\n",
    "\n",
    "\n",
    "# Convert to hierarchical JSON with user-defined levels\n",
    "hierarchical_json = df_to_hierarchical_json(melted_df, num_levels = column_index)\n",
    "\n",
    "# Store the hierarchical JSON into a file\n",
    "with open('New IndiaJuly.json', 'w') as file:\n",
    "    file.write(hierarchical_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melted_df['State (RTO)'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'variable_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'variable_1'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmelted_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvariable_1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39munique()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'variable_1'"
     ]
    }
   ],
   "source": [
    "melted_df['variable_1'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melted_df[(melted_df['State (RTO)']=='KA_Bangalore') & (melted_df['variable_3']== 'SCOOTER')]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
